{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9d73bb-e8d5-4aeb-a928-93db9ac6241c",
   "metadata": {
    "id": "2c9d73bb-e8d5-4aeb-a928-93db9ac6241c"
   },
   "source": [
    "# Transformer boilerplate code + how to use it\n",
    "##### by Daniel Melchor (dmh672@gmail.com)\n",
    "\n",
    "# https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31985af0-d58e-4d01-8432-065537022502",
   "metadata": {
    "id": "31985af0-d58e-4d01-8432-065537022502"
   },
   "source": [
    "---\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "446258e5-7cf2-4a73-8dd7-7b7a43fdd98e",
   "metadata": {
    "id": "446258e5-7cf2-4a73-8dd7-7b7a43fdd98e",
    "outputId": "47b53fa2-a24f-4b6a-baca-90d30bdb34a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3539f61f-fe7d-4428-b074-327a883f7f6e",
   "metadata": {
    "id": "3539f61f-fe7d-4428-b074-327a883f7f6e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c72c2ada-2106-4505-82f5-086e518080a5",
   "metadata": {
    "id": "c72c2ada-2106-4505-82f5-086e518080a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "588b2729-866c-470a-aca7-6a3c5ea0b192",
   "metadata": {
    "id": "588b2729-866c-470a-aca7-6a3c5ea0b192",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/p/c80afbc9ffb1/\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # LAYERS\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
    "        )\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout_p,\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, num_tokens)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "        \n",
    "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
    "        # to obtain size (sequence length, batch_size, dim_model),\n",
    "        src = src.permute(1,0,2)\n",
    "        tgt = tgt.permute(1,0,2)\n",
    "\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        out = self.out(transformer_out)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8ac39c4a-ab17-4487-a1ab-2b107e880574",
   "metadata": {
    "id": "8ac39c4a-ab17-4487-a1ab-2b107e880574",
    "outputId": "3ef96b46-f053-4d42-d2d2-c027c245f1d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562 batches of size 16\n",
      "187 batches of size 16\n"
     ]
    }
   ],
   "source": [
    "# If going from length 8 -> length 4, do SOS at [4] at put zeros for \n",
    "# the rest to keep length constant \n",
    "\n",
    "def generate_random_data(n):\n",
    "    SOS_token = np.array([8]) # For old, np.array([2])\n",
    "    EOS_token = np.array([9]) # For old, np.array([3])\n",
    "    length = 8 \n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i in range(n):\n",
    "        X = np.random.randint(0, 3, length)\n",
    "        y = np.zeros(length)\n",
    "        for i in range(int(8/2)):\n",
    "            y[i] = X[2*i] + X[2*i+1]\n",
    "        X = np.concatenate((SOS_token, X, EOS_token))\n",
    "        y = np.concatenate((SOS_token, y, EOS_token))\n",
    "        data.append([X, y])\n",
    "        \n",
    "    return data\n",
    "\n",
    "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
    "    batches = []\n",
    "    for idx in range(0, len(data), batch_size):\n",
    "        # We make sure we dont get the last bit if its not batch_size size\n",
    "        if idx + batch_size < len(data):\n",
    "            # Here you would need to get the max length of the batch,\n",
    "            # and normalize the length with the PAD token.\n",
    "            if padding:\n",
    "                max_batch_length = 0\n",
    "\n",
    "                # Get longest sentence in batch\n",
    "                for seq in data[idx : idx + batch_size]:\n",
    "                    if len(seq) > max_batch_length:\n",
    "                        max_batch_length = len(seq)\n",
    "\n",
    "                # Append X padding tokens until it reaches the max length\n",
    "                for seq_idx in range(batch_size):\n",
    "                    remaining_length = max_bath_length - len(data[idx + seq_idx])\n",
    "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
    "                    \n",
    "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
    "\n",
    "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "train_data = generate_random_data(9000)\n",
    "val_data = generate_random_data(3000)\n",
    "train_dataloader = batchify_data(train_data)\n",
    "val_dataloader = batchify_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6043e5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([8, 1, 1, 2, 1, 0, 2, 2, 0, 9]), array([8., 2., 3., 2., 2., 0., 0., 0., 0., 9.])]\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "data = generate_random_data(n)\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d04a60a-f013-4eda-b056-fc9bb1b1ea79",
   "metadata": {
    "id": "4d04a60a-f013-4eda-b056-fc9bb1b1ea79"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(\n",
    "    num_tokens=30, dim_model=8, num_heads=2, num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1\n",
    ").to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# CrossEntropyLoss -> may be typically used for categorization problems\n",
    "# Check more on that\n",
    "# Try MSE Loss next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7bc6949f-2d59-44c3-8198-037e115106aa",
   "metadata": {
    "id": "7bc6949f-2d59-44c3-8198-037e115106aa"
   },
   "outputs": [],
   "source": [
    "# Remember to dig into pred, y_input, and y_expected within training\n",
    "# and validation\n",
    "\n",
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch[:, 0], batch[:, 1]\n",
    "        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        pred = pred.permute(1, 2, 0)      \n",
    "        loss = loss_fn(pred, y_expected)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c23630c9-ce84-4996-8d8c-ff7919303331",
   "metadata": {
    "id": "c23630c9-ce84-4996-8d8c-ff7919303331"
   },
   "outputs": [],
   "source": [
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[:, 0], batch[:, 1]\n",
    "            X, y = torch.tensor(X, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:,:-1]\n",
    "            y_expected = y[:,1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "            # Standard training except we pass in y_input and src_mask\n",
    "            pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "            # Permute pred to have batch size first again\n",
    "            pred = pred.permute(1, 2, 0)      \n",
    "            loss = loss_fn(pred, y_expected)\n",
    "            total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5e57ba93-cc8f-4b12-870f-6aed4dce94ab",
   "metadata": {
    "id": "5e57ba93-cc8f-4b12-870f-6aed4dce94ab",
    "outputId": "da2b56bb-f094-45fe-efe3-76666acb68f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validating model\n",
      "------------------------- Epoch 1 -------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17028\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([16, 9])) that is different to the input size (torch.Size([16, 30, 9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (30) must match the size of tensor b (16) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27180/2391614102.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loss_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27180/2391614102.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, opt, loss_fn, train_dataloader, val_dataloader, epochs)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"Epoch {epoch + 1}\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27180/2359077347.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(model, opt, loss_fn, dataloader)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# Permute pred to have batch size first again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_expected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3109\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3111\u001b[1;33m     \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3112\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (30) must match the size of tensor b (16) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list\n",
    "    \n",
    "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader, val_dataloader, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "40f805ad-7748-4785-9066-c5500287a4af",
   "metadata": {
    "id": "40f805ad-7748-4785-9066-c5500287a4af",
    "outputId": "7d3e38be-0382-4311-9beb-15c60b3177d5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list, label = \"Train loss\")\n",
    "plt.plot(validation_loss_list, label = \"Validation loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5df24c30-15b0-471a-ac4d-a0975fbc6a7d",
   "metadata": {
    "id": "5df24c30-15b0-471a-ac4d-a0975fbc6a7d",
    "outputId": "7a7d4a5b-4f10-4cf4-cbe2-f204097bbf19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Input: [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Continuation: [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Example 2\n",
      "Input: [1, 2, 1, 1, 1, 1, 2, 1]\n",
      "Continuation: [3, 3, 3, 3, 3, 3, 3, 3]\n",
      "\n",
      "Example 3\n",
      "Input: [1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Continuation: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Example 4\n",
      "Input: [0, 1, 0, 1, 0, 1, 3, 1]\n",
      "Continuation: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Example 5\n",
      "Input: [0, 1, 2, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Continuation: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Example 6\n",
      "Input: [0, 0, 0, 1, 0, 0, 0, 1]\n",
      "Continuation: [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict(model, input_sequence, max_length=15, SOS_token=8, EOS_token=9):\n",
    "    model.eval()\n",
    "    \n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
    "    num_tokens = len(input_sequence[0])\n",
    "    \n",
    "    # Asks model to give only one item, the next thing it thinks is most probable \n",
    "    # to continue the sentence\n",
    "    for i in range(max_length):\n",
    "        # Get source mask\n",
    "        tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n",
    "        \n",
    "        pred = model(input_sequence, y_input, tgt_mask)\n",
    "        next_item = pred.topk(1)[1].view(-1)[0].item() # num with highest probability, twas view(-1)[-1] before\n",
    "        \n",
    "        ## Some wacky stuff to look at the inner workings :)\n",
    "        # print('The pred')\n",
    "        # print(pred)\n",
    "        # print('The pred.topk(2)')\n",
    "        # print(pred.topk(2))\n",
    "        # print('The pred.topk(2)[1]')\n",
    "        # print(pred.topk(2)[1])\n",
    "        # print('The pred.topk(2)[1].view(-1)')\n",
    "        # print(pred.topk(2)[1].view(-1))\n",
    "        # print('The pred.topk(2)[1].view(-1)[-1]')\n",
    "        # print(pred.topk(2)[1].view(-1)[-1])\n",
    "        \n",
    "        next_item = torch.tensor([[next_item]], device=device)\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "        elif (len(y_input.view(-1)) >= len(input_sequence[0])):\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()\n",
    "  \n",
    "  \n",
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([[8, 0, 0, 0, 0, 0, 0, 0, 0, 9]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[8, 1, 2, 1, 1, 1, 1, 2, 1, 9]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[8, 1, 0, 1, 0, 1, 0, 1, 0, 9]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[8, 0, 1, 0, 1, 0, 1, 3, 1, 9]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[8, 0, 1, 2, 1, 0, 1, 0, 1, 0, 1, 0, 9]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[8, 0, 0, 0, 1, 0, 0, 0, 1, 9]], dtype=torch.long, device=device)\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Example {idx+1}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408bbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "example_use.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
