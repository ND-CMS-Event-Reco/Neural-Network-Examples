{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' To make an end is to make a beginning\n",
    "\n",
    "\n",
    "    b a t c h  g r a d i e n t  d e s c e n t \n",
    "    \n",
    "    Play around with number of heads and number of layers; what is the minimum optimal number\n",
    "    \n",
    "    Give it unseen dataset within allowable set (i.e. give 0, 1, 0 instead of 2, 2, 2)\n",
    "    Go from 0-99 :)\n",
    "    Play around with model, embedding dimension\n",
    "    \n",
    "    Another test: keep 0-2, make length longer -->? random\n",
    "    Make random data set, much larger\n",
    "    \n",
    "    Does it learn length?\n",
    "        Can you feed it varying input lengths with corresponding output lengths and then learn how to do it itself  \n",
    "        \n",
    "    For actual physics: make embedding --> nn.Linear\n",
    "        Would this linear be an actual neural network\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataLoader():\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.target = []\n",
    "        self.sz = 0\n",
    "        \n",
    "    def read_data(self, file):\n",
    "        with open(file) as data_file:\n",
    "            data_reader = csv.reader(data_file)\n",
    "            for row in data_reader:\n",
    "                sequence = [int(row[0]), int(row[1]), int(row[2])]\n",
    "                self.data.append(sequence)\n",
    "        self.data = np.array(self.data)\n",
    "        \n",
    "    def get_target(self):\n",
    "        self.target = np.empty(self.data.shape)\n",
    "        self.target[:, 0] = self.data[:, 1] + self.data[:, 2]\n",
    "        self.target[:, 1] = self.data[:, 0] + self.data[:, 2]\n",
    "        self.target[:, 2] = self.data[:, 0] + self.data[:, 1]\n",
    "    \n",
    "    def tensorize(self):\n",
    "        SOS_token = 5\n",
    "        EOS_token = 6\n",
    "        self.data = torch.tensor(self.data).to(torch.long).to(device)\n",
    "        self.target = torch.tensor(self.target).to(torch.long).to(device)\n",
    "            \n",
    "        SOS_ = (torch.ones((self.data.shape[0], 1)) * SOS_token).to(torch.long).to(device)\n",
    "        EOS_ = (torch.ones((self.data.shape[0], 1)) * EOS_token).to(torch.long).to(device)\n",
    "        \n",
    "        self.data = torch.cat((SOS_, self.data, EOS_), 1).to(device)\n",
    "        self.target = torch.cat((SOS_, self.target, EOS_), 1).to(device)\n",
    "        \n",
    "    def sanity_check(self, index, verbose=True):\n",
    "        print(f'Data has shape of {self.data.shape}')\n",
    "        print(f'Target has shape of {self.target.shape}')\n",
    "        print(f'Compare data and target at index {index}: ')\n",
    "        print(f'    data[{index}] = {self.data[index]}')\n",
    "        print(f'    target[{index}] = {self.target[index]}')\n",
    "        \n",
    "        if verbose:\n",
    "            for index in range(1, len(self.data)+1):\n",
    "                print(f'I: {np.array(self.data[index-1].cpu())} --> O: {np.array(self.target[index-1].cpu())}  \\*/  ', end='')\n",
    "                if index != 0 and index % 2 == 0:\n",
    "                    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has shape of torch.Size([25, 5])\n",
      "Target has shape of torch.Size([25, 5])\n",
      "Compare data and target at index 2: \n",
      "    data[2] = tensor([5, 0, 0, 2, 6], device='cuda:0')\n",
      "    target[2] = tensor([5, 2, 2, 0, 6], device='cuda:0')\n",
      "I: [5 0 0 0 6] --> O: [5 0 0 0 6]  \\*/  I: [5 0 0 1 6] --> O: [5 1 1 0 6]  \\*/  \n",
      "\n",
      "I: [5 0 0 2 6] --> O: [5 2 2 0 6]  \\*/  I: [5 0 1 1 6] --> O: [5 2 1 1 6]  \\*/  \n",
      "\n",
      "I: [5 0 1 2 6] --> O: [5 3 2 1 6]  \\*/  I: [5 0 2 0 6] --> O: [5 2 0 2 6]  \\*/  \n",
      "\n",
      "I: [5 0 2 1 6] --> O: [5 3 1 2 6]  \\*/  I: [5 0 2 2 6] --> O: [5 4 2 2 6]  \\*/  \n",
      "\n",
      "I: [5 1 0 0 6] --> O: [5 0 1 1 6]  \\*/  I: [5 1 0 1 6] --> O: [5 1 2 1 6]  \\*/  \n",
      "\n",
      "I: [5 1 0 2 6] --> O: [5 2 3 1 6]  \\*/  I: [5 1 1 0 6] --> O: [5 1 1 2 6]  \\*/  \n",
      "\n",
      "I: [5 1 1 1 6] --> O: [5 2 2 2 6]  \\*/  I: [5 1 1 2 6] --> O: [5 3 3 2 6]  \\*/  \n",
      "\n",
      "I: [5 1 2 1 6] --> O: [5 3 2 3 6]  \\*/  I: [5 1 2 2 6] --> O: [5 4 3 3 6]  \\*/  \n",
      "\n",
      "I: [5 2 0 0 6] --> O: [5 0 2 2 6]  \\*/  I: [5 2 0 1 6] --> O: [5 1 3 2 6]  \\*/  \n",
      "\n",
      "I: [5 2 0 2 6] --> O: [5 2 4 2 6]  \\*/  I: [5 2 1 0 6] --> O: [5 1 2 3 6]  \\*/  \n",
      "\n",
      "I: [5 2 1 1 6] --> O: [5 2 3 3 6]  \\*/  I: [5 2 1 2 6] --> O: [5 3 4 3 6]  \\*/  \n",
      "\n",
      "I: [5 2 2 0 6] --> O: [5 2 2 4 6]  \\*/  I: [5 2 2 1 6] --> O: [5 3 3 4 6]  \\*/  \n",
      "\n",
      "I: [5 2 2 2 6] --> O: [5 4 4 4 6]  \\*/  "
     ]
    }
   ],
   "source": [
    "data_loader = dataLoader()\n",
    "data_loader.read_data('data.csv')\n",
    "data_loader.get_target()\n",
    "data_loader.tensorize()\n",
    "data_loader.sanity_check(2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "\n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, d_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.long).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, d_model, 2).long() * (-math.log(10000.0)) / d_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/d_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/d_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_token, d_model, n_head, n_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            d_model = d_model, \n",
    "            dropout_p = 0.1,\n",
    "            max_len = 100)\n",
    "        self.embedding = nn.Embedding(n_token, d_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers=n_layers,\n",
    "            num_decoder_layers=n_layers)\n",
    "        \n",
    "        # For floating point with one output, then self.out = nn.Linear(d_model, 1)\n",
    "        self.out = nn.Linear(d_model, n_token) # Learned linear at the end where output of decoder is run through\n",
    "        \n",
    "    def get_tgt_mask(self, size):\n",
    "    \n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \n",
    "        src = (self.embedding(src) * math.sqrt(self.d_model)).to(device)\n",
    "        tgt = (self.embedding(tgt) * math.sqrt(self.d_model)).to(device)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "        \n",
    "        src = src.permute(1, 0, 2)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        \n",
    "        transformer_output = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask).to(device)\n",
    "        output = self.out(transformer_output).to(device)\n",
    "        \n",
    "        output = output.permute(1, 2, 0).to(device)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens are 7, six categories to calculate probability distributions over (i.e. 0, 1, 2, 3, 4 + sos + eos)\n",
      "src:\n",
      "  Size: torch.Size([2, 3])\n",
      "  Tensor: tensor([[0, 0, 0],\n",
      "        [1, 1, 0]], device='cuda:0')\n",
      "tgt:\n",
      "  Size: torch.Size([2, 3])\n",
      "  Tensor: tensor([[0, 0, 0],\n",
      "        [1, 1, 0]], device='cuda:0')\n",
      "output:\n",
      "  Size: torch.Size([2, 7, 3])\n",
      "  Tensor: tensor([[[-0.7346, -0.9109, -0.8613],\n",
      "         [-0.1324, -0.1715,  0.0156],\n",
      "         [-0.0171,  0.4595,  0.4978],\n",
      "         [-0.0071,  0.1696,  0.0319],\n",
      "         [ 0.1899,  0.5474,  1.2257],\n",
      "         [-0.2732,  0.2747, -0.0718],\n",
      "         [ 0.3814,  0.5105,  0.4273]],\n",
      "\n",
      "        [[-0.5216, -0.0158, -0.1174],\n",
      "         [-0.6977, -0.5814, -0.6106],\n",
      "         [ 0.4445,  0.3799,  0.8668],\n",
      "         [-0.9561, -0.5316, -0.4545],\n",
      "         [ 0.0316,  0.2670, -0.3350],\n",
      "         [-0.9354, -0.4088, -0.8884],\n",
      "         [-0.9278, -0.5854, -0.5199]]], device='cuda:0',\n",
      "       grad_fn=<PermuteBackward>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "src:\n",
      "  Size: torch.Size([2, 3])\n",
      "  Tensor: tensor([[1, 0, 0],\n",
      "        [0, 0, 1]], device='cuda:0')\n",
      "tgt_input:\n",
      "  Size: torch.Size([2, 2])\n",
      "  Tensor: tensor([[1, 0],\n",
      "        [0, 0]], device='cuda:0')\n",
      "output:\n",
      "  Size: torch.Size([2, 7, 2])\n",
      "  Tensor: tensor([[[-0.9511, -0.9547],\n",
      "         [-0.6358, -0.5504],\n",
      "         [ 0.1413,  0.4067],\n",
      "         [-0.4881, -0.6397],\n",
      "         [ 0.1141,  0.0432],\n",
      "         [ 0.0391,  0.3255],\n",
      "         [ 0.1314,  0.2571]],\n",
      "\n",
      "        [[-0.6368,  0.0732],\n",
      "         [-0.3099, -0.7995],\n",
      "         [ 0.4567,  0.2561],\n",
      "         [ 0.3110,  0.3013],\n",
      "         [-0.2244, -0.5040],\n",
      "         [-0.3680,  0.3197],\n",
      "         [-0.0253, -0.0477]]], device='cuda:0', grad_fn=<PermuteBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Let's define an instance :)\n",
    "d_model = 60 # Let's try embedding brotha\n",
    "n_token = 7 # Only do 0, 1, 2?\n",
    "n_head = 3 # Remember that d_model=d_embedding must be divisible by number of n_head\n",
    "n_layers = 4\n",
    "\n",
    "model = TransformerModel(n_token, d_model, n_head, n_layers).to(device)\n",
    "\n",
    "print(f'The number of tokens are {n_token}, six categories to calculate probability distributions over (i.e. 0, 1, 2, 3, 4 + sos + eos)')\n",
    "\n",
    "# Check da dimension\n",
    "src = torch.randint(size=(2, 3), low=0, high=2).to(device)\n",
    "tgt = src.to(device)\n",
    "\n",
    "output = model(src, src).to(device)\n",
    "print(f'src:\\n  Size: {src.size()}\\n  Tensor: {src}')\n",
    "print(f'tgt:\\n  Size: {tgt.size()}\\n  Tensor: {tgt}')\n",
    "print(f'output:\\n  Size: {output.size()}\\n  Tensor: {output}\\n')\n",
    "\n",
    "src = torch.randint(size=(2, 3), low=0, high=2).to(device)\n",
    "tgt = src.to(device)\n",
    "tgt_input = tgt[:, :-1].to(device)\n",
    "tgt_expected = tgt[:, 1:].to(device)\n",
    "output = model(src, tgt_input).to(device)\n",
    "\n",
    "print('----------------------------------------------------------------------------')\n",
    "print(f'src:\\n  Size: {src.size()}\\n  Tensor: {src}')\n",
    "print(f'tgt_input:\\n  Size: {tgt_input.size()}\\n  Tensor: {tgt_input}')\n",
    "print(f'output:\\n  Size: {output.size()}\\n  Tensor: {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# America needs more trains\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, data, target):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    data = data.type(torch.long).to(device)\n",
    "    target = target.type(torch.long).to(device)\n",
    "          \n",
    "    target_in = target[:, :-1]\n",
    "    target_expected = target[:, 1:]\n",
    "\n",
    "    sequence_length = target_in.size(1)\n",
    "    \n",
    "    tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "    src_mask = model.get_tgt_mask(data.size(1)).to(device)\n",
    "    pred = model(data, target_in, src_mask, tgt_mask)\n",
    "    \n",
    "    loss = (loss_fn(pred, target_expected)).type(torch.float)\n",
    "        \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss\n",
    "    \n",
    "def train_loop(model, n_epochs, data, target):\n",
    "    best_loss = 10000.0\n",
    "    best_model = copy.deepcopy(model).to(device)\n",
    "    loss_ = np.array([])\n",
    "    epochs = np.array([])\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        loss = train(model, data, target)\n",
    "        loss_ = np.append(loss_, loss)\n",
    "        epochs = np.append(epochs, i)\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = copy.deepcopy(model).to(device)\n",
    "                     \n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {i}\\nTotal Loss: {loss}')\n",
    "            print(f'----------------------------------')\n",
    "    \n",
    "    return best_model, loss_, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Total Loss: 1.918920636177063\n",
      "----------------------------------\n",
      "Epoch: 10\n",
      "Total Loss: 1.7440413236618042\n",
      "----------------------------------\n",
      "Epoch: 20\n",
      "Total Loss: 1.6725871562957764\n",
      "----------------------------------\n",
      "Epoch: 30\n",
      "Total Loss: 1.564504861831665\n",
      "----------------------------------\n",
      "Epoch: 40\n",
      "Total Loss: 1.468981146812439\n",
      "----------------------------------\n",
      "Epoch: 50\n",
      "Total Loss: 1.3954474925994873\n",
      "----------------------------------\n",
      "Epoch: 60\n",
      "Total Loss: 1.336423635482788\n",
      "----------------------------------\n",
      "Epoch: 70\n",
      "Total Loss: 1.2528495788574219\n",
      "----------------------------------\n",
      "Epoch: 80\n",
      "Total Loss: 1.2248183488845825\n",
      "----------------------------------\n",
      "Epoch: 90\n",
      "Total Loss: 1.1895867586135864\n",
      "----------------------------------\n",
      "Epoch: 100\n",
      "Total Loss: 1.1719690561294556\n",
      "----------------------------------\n",
      "Epoch: 110\n",
      "Total Loss: 1.095517873764038\n",
      "----------------------------------\n",
      "Epoch: 120\n",
      "Total Loss: 1.099975347518921\n",
      "----------------------------------\n",
      "Epoch: 130\n",
      "Total Loss: 1.0150823593139648\n",
      "----------------------------------\n",
      "Epoch: 140\n",
      "Total Loss: 1.0479018688201904\n",
      "----------------------------------\n",
      "Epoch: 150\n",
      "Total Loss: 0.9986618757247925\n",
      "----------------------------------\n",
      "Epoch: 160\n",
      "Total Loss: 0.9766319394111633\n",
      "----------------------------------\n",
      "Epoch: 170\n",
      "Total Loss: 0.924162745475769\n",
      "----------------------------------\n",
      "Epoch: 180\n",
      "Total Loss: 0.9262177348136902\n",
      "----------------------------------\n",
      "Epoch: 190\n",
      "Total Loss: 0.9207972884178162\n",
      "----------------------------------\n",
      "Epoch: 200\n",
      "Total Loss: 0.9083482623100281\n",
      "----------------------------------\n",
      "Epoch: 210\n",
      "Total Loss: 0.7742587924003601\n",
      "----------------------------------\n",
      "Epoch: 220\n",
      "Total Loss: 0.8173863887786865\n",
      "----------------------------------\n",
      "Epoch: 230\n",
      "Total Loss: 0.7770598530769348\n",
      "----------------------------------\n",
      "Epoch: 240\n",
      "Total Loss: 0.7073663473129272\n",
      "----------------------------------\n",
      "Epoch: 250\n",
      "Total Loss: 0.770795464515686\n",
      "----------------------------------\n",
      "Epoch: 260\n",
      "Total Loss: 0.6756702661514282\n",
      "----------------------------------\n",
      "Epoch: 270\n",
      "Total Loss: 0.7266873121261597\n",
      "----------------------------------\n",
      "Epoch: 280\n",
      "Total Loss: 0.6767147183418274\n",
      "----------------------------------\n",
      "Epoch: 290\n",
      "Total Loss: 0.7193780541419983\n",
      "----------------------------------\n",
      "Epoch: 300\n",
      "Total Loss: 0.6749880909919739\n",
      "----------------------------------\n",
      "Epoch: 310\n",
      "Total Loss: 0.6815357208251953\n",
      "----------------------------------\n",
      "Epoch: 320\n",
      "Total Loss: 0.609300434589386\n",
      "----------------------------------\n",
      "Epoch: 330\n",
      "Total Loss: 0.6091238260269165\n",
      "----------------------------------\n",
      "Epoch: 340\n",
      "Total Loss: 0.5214287638664246\n",
      "----------------------------------\n",
      "Epoch: 350\n",
      "Total Loss: 0.6268932223320007\n",
      "----------------------------------\n",
      "Epoch: 360\n",
      "Total Loss: 0.5825399160385132\n",
      "----------------------------------\n",
      "Epoch: 370\n",
      "Total Loss: 0.5385282039642334\n",
      "----------------------------------\n",
      "Epoch: 380\n",
      "Total Loss: 0.5580608248710632\n",
      "----------------------------------\n",
      "Epoch: 390\n",
      "Total Loss: 0.5587048530578613\n",
      "----------------------------------\n",
      "Epoch: 400\n",
      "Total Loss: 0.5251575112342834\n",
      "----------------------------------\n",
      "Epoch: 410\n",
      "Total Loss: 0.47633078694343567\n",
      "----------------------------------\n",
      "Epoch: 420\n",
      "Total Loss: 0.500597357749939\n",
      "----------------------------------\n",
      "Epoch: 430\n",
      "Total Loss: 0.43078187108039856\n",
      "----------------------------------\n",
      "Epoch: 440\n",
      "Total Loss: 0.4522424042224884\n",
      "----------------------------------\n",
      "Epoch: 450\n",
      "Total Loss: 0.5417267680168152\n",
      "----------------------------------\n",
      "Epoch: 460\n",
      "Total Loss: 0.5160359144210815\n",
      "----------------------------------\n",
      "Epoch: 470\n",
      "Total Loss: 0.4759838879108429\n",
      "----------------------------------\n",
      "Epoch: 480\n",
      "Total Loss: 0.5462742447853088\n",
      "----------------------------------\n",
      "Epoch: 490\n",
      "Total Loss: 0.4562736749649048\n",
      "----------------------------------\n",
      "Epoch: 500\n",
      "Total Loss: 0.43054094910621643\n",
      "----------------------------------\n",
      "Epoch: 510\n",
      "Total Loss: 0.4479902684688568\n",
      "----------------------------------\n",
      "Epoch: 520\n",
      "Total Loss: 0.5416787266731262\n",
      "----------------------------------\n",
      "Epoch: 530\n",
      "Total Loss: 0.3886853754520416\n",
      "----------------------------------\n",
      "Epoch: 540\n",
      "Total Loss: 0.43074145913124084\n",
      "----------------------------------\n",
      "Epoch: 550\n",
      "Total Loss: 0.4139835834503174\n",
      "----------------------------------\n",
      "Epoch: 560\n",
      "Total Loss: 0.4243182837963104\n",
      "----------------------------------\n",
      "Epoch: 570\n",
      "Total Loss: 0.3535200357437134\n",
      "----------------------------------\n",
      "Epoch: 580\n",
      "Total Loss: 0.38240790367126465\n",
      "----------------------------------\n",
      "Epoch: 590\n",
      "Total Loss: 0.4456685185432434\n",
      "----------------------------------\n",
      "Epoch: 600\n",
      "Total Loss: 0.4328647553920746\n",
      "----------------------------------\n",
      "Epoch: 610\n",
      "Total Loss: 0.37116891145706177\n",
      "----------------------------------\n",
      "Epoch: 620\n",
      "Total Loss: 0.3654536306858063\n",
      "----------------------------------\n",
      "Epoch: 630\n",
      "Total Loss: 0.32785770297050476\n",
      "----------------------------------\n",
      "Epoch: 640\n",
      "Total Loss: 0.42680031061172485\n",
      "----------------------------------\n",
      "Epoch: 650\n",
      "Total Loss: 0.3025142252445221\n",
      "----------------------------------\n",
      "Epoch: 660\n",
      "Total Loss: 0.3347949981689453\n",
      "----------------------------------\n",
      "Epoch: 670\n",
      "Total Loss: 0.36017051339149475\n",
      "----------------------------------\n",
      "Epoch: 680\n",
      "Total Loss: 0.35185855627059937\n",
      "----------------------------------\n",
      "Epoch: 690\n",
      "Total Loss: 0.36246412992477417\n",
      "----------------------------------\n",
      "Epoch: 700\n",
      "Total Loss: 0.3980139195919037\n",
      "----------------------------------\n",
      "Epoch: 710\n",
      "Total Loss: 0.330577552318573\n",
      "----------------------------------\n",
      "Epoch: 720\n",
      "Total Loss: 0.39076849818229675\n",
      "----------------------------------\n",
      "Epoch: 730\n",
      "Total Loss: 0.3014640212059021\n",
      "----------------------------------\n",
      "Epoch: 740\n",
      "Total Loss: 0.3146637976169586\n",
      "----------------------------------\n",
      "Epoch: 750\n",
      "Total Loss: 0.28796011209487915\n",
      "----------------------------------\n",
      "Epoch: 760\n",
      "Total Loss: 0.2891988456249237\n",
      "----------------------------------\n",
      "Epoch: 770\n",
      "Total Loss: 0.33812087774276733\n",
      "----------------------------------\n",
      "Epoch: 780\n",
      "Total Loss: 0.2654013931751251\n",
      "----------------------------------\n",
      "Epoch: 790\n",
      "Total Loss: 0.3388121724128723\n",
      "----------------------------------\n",
      "Epoch: 800\n",
      "Total Loss: 0.3412213921546936\n",
      "----------------------------------\n",
      "Epoch: 810\n",
      "Total Loss: 0.3143390119075775\n",
      "----------------------------------\n",
      "Epoch: 820\n",
      "Total Loss: 0.318027138710022\n",
      "----------------------------------\n",
      "Epoch: 830\n",
      "Total Loss: 0.31853657960891724\n",
      "----------------------------------\n",
      "Epoch: 840\n",
      "Total Loss: 0.24414846301078796\n",
      "----------------------------------\n",
      "Epoch: 850\n",
      "Total Loss: 0.3314732313156128\n",
      "----------------------------------\n",
      "Epoch: 860\n",
      "Total Loss: 0.2882409393787384\n",
      "----------------------------------\n",
      "Epoch: 870\n",
      "Total Loss: 0.18968498706817627\n",
      "----------------------------------\n",
      "Epoch: 880\n",
      "Total Loss: 0.2637518048286438\n",
      "----------------------------------\n",
      "Epoch: 890\n",
      "Total Loss: 0.24632903933525085\n",
      "----------------------------------\n",
      "Epoch: 900\n",
      "Total Loss: 0.1923435777425766\n",
      "----------------------------------\n",
      "Epoch: 910\n",
      "Total Loss: 0.2550172507762909\n",
      "----------------------------------\n",
      "Epoch: 920\n",
      "Total Loss: 0.2243800163269043\n",
      "----------------------------------\n",
      "Epoch: 930\n",
      "Total Loss: 0.271295964717865\n",
      "----------------------------------\n",
      "Epoch: 940\n",
      "Total Loss: 0.24513503909111023\n",
      "----------------------------------\n",
      "Epoch: 950\n",
      "Total Loss: 0.2721290588378906\n",
      "----------------------------------\n",
      "Epoch: 960\n",
      "Total Loss: 0.2974056899547577\n",
      "----------------------------------\n",
      "Epoch: 970\n",
      "Total Loss: 0.27966251969337463\n",
      "----------------------------------\n",
      "Epoch: 980\n",
      "Total Loss: 0.3113521337509155\n",
      "----------------------------------\n",
      "Epoch: 990\n",
      "Total Loss: 0.2049252688884735\n",
      "----------------------------------\n",
      "Epoch: 1000\n",
      "Total Loss: 0.2318231761455536\n",
      "----------------------------------\n",
      "Epoch: 1010\n",
      "Total Loss: 0.22198516130447388\n",
      "----------------------------------\n",
      "Epoch: 1020\n",
      "Total Loss: 0.19194164872169495\n",
      "----------------------------------\n",
      "Epoch: 1030\n",
      "Total Loss: 0.18142735958099365\n",
      "----------------------------------\n",
      "Epoch: 1040\n",
      "Total Loss: 0.30971407890319824\n",
      "----------------------------------\n",
      "Epoch: 1050\n",
      "Total Loss: 0.26571303606033325\n",
      "----------------------------------\n",
      "Epoch: 1060\n",
      "Total Loss: 0.238159641623497\n",
      "----------------------------------\n",
      "Epoch: 1070\n",
      "Total Loss: 0.1415528506040573\n",
      "----------------------------------\n",
      "Epoch: 1080\n",
      "Total Loss: 0.33574020862579346\n",
      "----------------------------------\n",
      "Epoch: 1090\n",
      "Total Loss: 0.19401516020298004\n",
      "----------------------------------\n",
      "Epoch: 1100\n",
      "Total Loss: 0.1921565979719162\n",
      "----------------------------------\n",
      "Epoch: 1110\n",
      "Total Loss: 0.21785376965999603\n",
      "----------------------------------\n",
      "Epoch: 1120\n",
      "Total Loss: 0.1537625640630722\n",
      "----------------------------------\n",
      "Epoch: 1130\n",
      "Total Loss: 0.19268690049648285\n",
      "----------------------------------\n",
      "Epoch: 1140\n",
      "Total Loss: 0.1734079122543335\n",
      "----------------------------------\n",
      "Epoch: 1150\n",
      "Total Loss: 0.1971614956855774\n",
      "----------------------------------\n",
      "Epoch: 1160\n",
      "Total Loss: 0.16670532524585724\n",
      "----------------------------------\n",
      "Epoch: 1170\n",
      "Total Loss: 0.211568221449852\n",
      "----------------------------------\n",
      "Epoch: 1180\n",
      "Total Loss: 0.26141053438186646\n",
      "----------------------------------\n",
      "Epoch: 1190\n",
      "Total Loss: 0.21829243004322052\n",
      "----------------------------------\n",
      "Epoch: 1200\n",
      "Total Loss: 0.1747402399778366\n",
      "----------------------------------\n",
      "Epoch: 1210\n",
      "Total Loss: 0.18048927187919617\n",
      "----------------------------------\n",
      "Epoch: 1220\n",
      "Total Loss: 0.17244872450828552\n",
      "----------------------------------\n",
      "Epoch: 1230\n",
      "Total Loss: 0.1794613003730774\n",
      "----------------------------------\n",
      "Epoch: 1240\n",
      "Total Loss: 0.20579874515533447\n",
      "----------------------------------\n",
      "Epoch: 1250\n",
      "Total Loss: 0.13697907328605652\n",
      "----------------------------------\n",
      "Epoch: 1260\n",
      "Total Loss: 0.18115799129009247\n",
      "----------------------------------\n",
      "Epoch: 1270\n",
      "Total Loss: 0.2017820030450821\n",
      "----------------------------------\n",
      "Epoch: 1280\n",
      "Total Loss: 0.16069549322128296\n",
      "----------------------------------\n",
      "Epoch: 1290\n",
      "Total Loss: 0.16057363152503967\n",
      "----------------------------------\n",
      "Epoch: 1300\n",
      "Total Loss: 0.18220417201519012\n",
      "----------------------------------\n",
      "Epoch: 1310\n",
      "Total Loss: 0.2409828156232834\n",
      "----------------------------------\n",
      "Epoch: 1320\n",
      "Total Loss: 0.24192047119140625\n",
      "----------------------------------\n",
      "Epoch: 1330\n",
      "Total Loss: 0.2236437350511551\n",
      "----------------------------------\n",
      "Epoch: 1340\n",
      "Total Loss: 0.15010644495487213\n",
      "----------------------------------\n",
      "Epoch: 1350\n",
      "Total Loss: 0.1284964680671692\n",
      "----------------------------------\n",
      "Epoch: 1360\n",
      "Total Loss: 0.10139039903879166\n",
      "----------------------------------\n",
      "Epoch: 1370\n",
      "Total Loss: 0.17333389818668365\n",
      "----------------------------------\n",
      "Epoch: 1380\n",
      "Total Loss: 0.21633461117744446\n",
      "----------------------------------\n",
      "Epoch: 1390\n",
      "Total Loss: 0.12764203548431396\n",
      "----------------------------------\n",
      "Epoch: 1400\n",
      "Total Loss: 0.13570985198020935\n",
      "----------------------------------\n",
      "Epoch: 1410\n",
      "Total Loss: 0.10682521760463715\n",
      "----------------------------------\n",
      "Epoch: 1420\n",
      "Total Loss: 0.12884162366390228\n",
      "----------------------------------\n",
      "Epoch: 1430\n",
      "Total Loss: 0.17308275401592255\n",
      "----------------------------------\n",
      "Epoch: 1440\n",
      "Total Loss: 0.08463317900896072\n",
      "----------------------------------\n",
      "Epoch: 1450\n",
      "Total Loss: 0.1061113178730011\n",
      "----------------------------------\n",
      "Epoch: 1460\n",
      "Total Loss: 0.10864481329917908\n",
      "----------------------------------\n",
      "Epoch: 1470\n",
      "Total Loss: 0.11878057569265366\n",
      "----------------------------------\n",
      "Epoch: 1480\n",
      "Total Loss: 0.18566787242889404\n",
      "----------------------------------\n",
      "Epoch: 1490\n",
      "Total Loss: 0.13605566322803497\n",
      "----------------------------------\n",
      "Epoch: 1500\n",
      "Total Loss: 0.15250736474990845\n",
      "----------------------------------\n",
      "Epoch: 1510\n",
      "Total Loss: 0.1651218831539154\n",
      "----------------------------------\n",
      "Epoch: 1520\n",
      "Total Loss: 0.07854410260915756\n",
      "----------------------------------\n",
      "Epoch: 1530\n",
      "Total Loss: 0.16154049336910248\n",
      "----------------------------------\n",
      "Epoch: 1540\n",
      "Total Loss: 0.1762557029724121\n",
      "----------------------------------\n",
      "Epoch: 1550\n",
      "Total Loss: 0.15542364120483398\n",
      "----------------------------------\n",
      "Epoch: 1560\n",
      "Total Loss: 0.12252511829137802\n",
      "----------------------------------\n",
      "Epoch: 1570\n",
      "Total Loss: 0.1516435444355011\n",
      "----------------------------------\n",
      "Epoch: 1580\n",
      "Total Loss: 0.1524774432182312\n",
      "----------------------------------\n",
      "Epoch: 1590\n",
      "Total Loss: 0.1339484453201294\n",
      "----------------------------------\n",
      "Epoch: 1600\n",
      "Total Loss: 0.08108124136924744\n",
      "----------------------------------\n",
      "Epoch: 1610\n",
      "Total Loss: 0.17904286086559296\n",
      "----------------------------------\n",
      "Epoch: 1620\n",
      "Total Loss: 0.12556026875972748\n",
      "----------------------------------\n",
      "Epoch: 1630\n",
      "Total Loss: 0.1473541110754013\n",
      "----------------------------------\n",
      "Epoch: 1640\n",
      "Total Loss: 0.1397155076265335\n",
      "----------------------------------\n",
      "Epoch: 1650\n",
      "Total Loss: 0.136636883020401\n",
      "----------------------------------\n",
      "Epoch: 1660\n",
      "Total Loss: 0.11824718117713928\n",
      "----------------------------------\n",
      "Epoch: 1670\n",
      "Total Loss: 0.07791920006275177\n",
      "----------------------------------\n",
      "Epoch: 1680\n",
      "Total Loss: 0.17753465473651886\n",
      "----------------------------------\n",
      "Epoch: 1690\n",
      "Total Loss: 0.1020921915769577\n",
      "----------------------------------\n",
      "Epoch: 1700\n",
      "Total Loss: 0.15529653429985046\n",
      "----------------------------------\n",
      "Epoch: 1710\n",
      "Total Loss: 0.12107469886541367\n",
      "----------------------------------\n",
      "Epoch: 1720\n",
      "Total Loss: 0.13238243758678436\n",
      "----------------------------------\n",
      "Epoch: 1730\n",
      "Total Loss: 0.16244268417358398\n",
      "----------------------------------\n",
      "Epoch: 1740\n",
      "Total Loss: 0.1762886494398117\n",
      "----------------------------------\n",
      "Epoch: 1750\n",
      "Total Loss: 0.09384910762310028\n",
      "----------------------------------\n",
      "Epoch: 1760\n",
      "Total Loss: 0.10730168223381042\n",
      "----------------------------------\n",
      "Epoch: 1770\n",
      "Total Loss: 0.14893454313278198\n",
      "----------------------------------\n",
      "Epoch: 1780\n",
      "Total Loss: 0.1436852663755417\n",
      "----------------------------------\n",
      "Epoch: 1790\n",
      "Total Loss: 0.1903892308473587\n",
      "----------------------------------\n",
      "Epoch: 1800\n",
      "Total Loss: 0.15062208473682404\n",
      "----------------------------------\n",
      "Epoch: 1810\n",
      "Total Loss: 0.12510931491851807\n",
      "----------------------------------\n",
      "Epoch: 1820\n",
      "Total Loss: 0.18522262573242188\n",
      "----------------------------------\n",
      "Epoch: 1830\n",
      "Total Loss: 0.10347307473421097\n",
      "----------------------------------\n",
      "Epoch: 1840\n",
      "Total Loss: 0.12054099887609482\n",
      "----------------------------------\n",
      "Epoch: 1850\n",
      "Total Loss: 0.1111915111541748\n",
      "----------------------------------\n",
      "Epoch: 1860\n",
      "Total Loss: 0.17314019799232483\n",
      "----------------------------------\n",
      "Epoch: 1870\n",
      "Total Loss: 0.14079931378364563\n",
      "----------------------------------\n",
      "Epoch: 1880\n",
      "Total Loss: 0.18252940475940704\n",
      "----------------------------------\n",
      "Epoch: 1890\n",
      "Total Loss: 0.11374345421791077\n",
      "----------------------------------\n",
      "Epoch: 1900\n",
      "Total Loss: 0.0922701433300972\n",
      "----------------------------------\n",
      "Epoch: 1910\n",
      "Total Loss: 0.15831464529037476\n",
      "----------------------------------\n",
      "Epoch: 1920\n",
      "Total Loss: 0.06832201778888702\n",
      "----------------------------------\n",
      "Epoch: 1930\n",
      "Total Loss: 0.10542763024568558\n",
      "----------------------------------\n",
      "Epoch: 1940\n",
      "Total Loss: 0.05666667968034744\n",
      "----------------------------------\n",
      "Epoch: 1950\n",
      "Total Loss: 0.09603211283683777\n",
      "----------------------------------\n",
      "Epoch: 1960\n",
      "Total Loss: 0.09509964287281036\n",
      "----------------------------------\n",
      "Epoch: 1970\n",
      "Total Loss: 0.12380381673574448\n",
      "----------------------------------\n",
      "Epoch: 1980\n",
      "Total Loss: 0.1244882196187973\n",
      "----------------------------------\n",
      "Epoch: 1990\n",
      "Total Loss: 0.09979024529457092\n",
      "----------------------------------\n",
      "Epoch: 2000\n",
      "Total Loss: 0.10757265239953995\n",
      "----------------------------------\n",
      "Epoch: 2010\n",
      "Total Loss: 0.08729956299066544\n",
      "----------------------------------\n",
      "Epoch: 2020\n",
      "Total Loss: 0.09430453926324844\n",
      "----------------------------------\n",
      "Epoch: 2030\n",
      "Total Loss: 0.05979354679584503\n",
      "----------------------------------\n",
      "Epoch: 2040\n",
      "Total Loss: 0.12071939557790756\n",
      "----------------------------------\n",
      "Epoch: 2050\n",
      "Total Loss: 0.09062055498361588\n",
      "----------------------------------\n",
      "Epoch: 2060\n",
      "Total Loss: 0.06203755736351013\n",
      "----------------------------------\n",
      "Epoch: 2070\n",
      "Total Loss: 0.07292881608009338\n",
      "----------------------------------\n",
      "Epoch: 2080\n",
      "Total Loss: 0.20697879791259766\n",
      "----------------------------------\n",
      "Epoch: 2090\n",
      "Total Loss: 0.10480993241071701\n",
      "----------------------------------\n",
      "Epoch: 2100\n",
      "Total Loss: 0.09641948342323303\n",
      "----------------------------------\n",
      "Epoch: 2110\n",
      "Total Loss: 0.08056104928255081\n",
      "----------------------------------\n",
      "Epoch: 2120\n",
      "Total Loss: 0.07524783164262772\n",
      "----------------------------------\n",
      "Epoch: 2130\n",
      "Total Loss: 0.08979091793298721\n",
      "----------------------------------\n",
      "Epoch: 2140\n",
      "Total Loss: 0.14084218442440033\n",
      "----------------------------------\n",
      "Epoch: 2150\n",
      "Total Loss: 0.18659572303295135\n",
      "----------------------------------\n",
      "Epoch: 2160\n",
      "Total Loss: 0.15687185525894165\n",
      "----------------------------------\n",
      "Epoch: 2170\n",
      "Total Loss: 0.0770265981554985\n",
      "----------------------------------\n",
      "Epoch: 2180\n",
      "Total Loss: 0.1017100140452385\n",
      "----------------------------------\n",
      "Epoch: 2190\n",
      "Total Loss: 0.050556544214487076\n",
      "----------------------------------\n",
      "Epoch: 2200\n",
      "Total Loss: 0.12357786297798157\n",
      "----------------------------------\n",
      "Epoch: 2210\n",
      "Total Loss: 0.06907995045185089\n",
      "----------------------------------\n",
      "Epoch: 2220\n",
      "Total Loss: 0.15106914937496185\n",
      "----------------------------------\n",
      "Epoch: 2230\n",
      "Total Loss: 0.12249084562063217\n",
      "----------------------------------\n",
      "Epoch: 2240\n",
      "Total Loss: 0.16686636209487915\n",
      "----------------------------------\n",
      "Epoch: 2250\n",
      "Total Loss: 0.13229981064796448\n",
      "----------------------------------\n",
      "Epoch: 2260\n",
      "Total Loss: 0.09277059882879257\n",
      "----------------------------------\n",
      "Epoch: 2270\n",
      "Total Loss: 0.08921891450881958\n",
      "----------------------------------\n",
      "Epoch: 2280\n",
      "Total Loss: 0.05107787251472473\n",
      "----------------------------------\n",
      "Epoch: 2290\n",
      "Total Loss: 0.05416823551058769\n",
      "----------------------------------\n",
      "Epoch: 2300\n",
      "Total Loss: 0.07819958031177521\n",
      "----------------------------------\n",
      "Epoch: 2310\n",
      "Total Loss: 0.06528347730636597\n",
      "----------------------------------\n",
      "Epoch: 2320\n",
      "Total Loss: 0.10034492611885071\n",
      "----------------------------------\n",
      "Epoch: 2330\n",
      "Total Loss: 0.16062885522842407\n",
      "----------------------------------\n",
      "Epoch: 2340\n",
      "Total Loss: 0.08205519616603851\n",
      "----------------------------------\n",
      "Epoch: 2350\n",
      "Total Loss: 0.06405872106552124\n",
      "----------------------------------\n",
      "Epoch: 2360\n",
      "Total Loss: 0.05318060889840126\n",
      "----------------------------------\n",
      "Epoch: 2370\n",
      "Total Loss: 0.07750479876995087\n",
      "----------------------------------\n",
      "Epoch: 2380\n",
      "Total Loss: 0.11230602115392685\n",
      "----------------------------------\n",
      "Epoch: 2390\n",
      "Total Loss: 0.06866701692342758\n",
      "----------------------------------\n",
      "Epoch: 2400\n",
      "Total Loss: 0.1133418083190918\n",
      "----------------------------------\n",
      "Epoch: 2410\n",
      "Total Loss: 0.12784932553768158\n",
      "----------------------------------\n",
      "Epoch: 2420\n",
      "Total Loss: 0.10938920825719833\n",
      "----------------------------------\n",
      "Epoch: 2430\n",
      "Total Loss: 0.0726630911231041\n",
      "----------------------------------\n",
      "Epoch: 2440\n",
      "Total Loss: 0.07223568111658096\n",
      "----------------------------------\n",
      "Epoch: 2450\n",
      "Total Loss: 0.0578487291932106\n",
      "----------------------------------\n",
      "Epoch: 2460\n",
      "Total Loss: 0.05006115511059761\n",
      "----------------------------------\n",
      "Epoch: 2470\n",
      "Total Loss: 0.10792171210050583\n",
      "----------------------------------\n",
      "Epoch: 2480\n",
      "Total Loss: 0.10493704676628113\n",
      "----------------------------------\n",
      "Epoch: 2490\n",
      "Total Loss: 0.11976422369480133\n",
      "----------------------------------\n",
      "Epoch: 2500\n",
      "Total Loss: 0.10521826893091202\n",
      "----------------------------------\n",
      "Epoch: 2510\n",
      "Total Loss: 0.05529109388589859\n",
      "----------------------------------\n",
      "Epoch: 2520\n",
      "Total Loss: 0.10675744712352753\n",
      "----------------------------------\n",
      "Epoch: 2530\n",
      "Total Loss: 0.06133907288312912\n",
      "----------------------------------\n",
      "Epoch: 2540\n",
      "Total Loss: 0.07984963059425354\n",
      "----------------------------------\n",
      "Epoch: 2550\n",
      "Total Loss: 0.07763459533452988\n",
      "----------------------------------\n",
      "Epoch: 2560\n",
      "Total Loss: 0.08308709412813187\n",
      "----------------------------------\n",
      "Epoch: 2570\n",
      "Total Loss: 0.08130605518817902\n",
      "----------------------------------\n",
      "Epoch: 2580\n",
      "Total Loss: 0.150316521525383\n",
      "----------------------------------\n",
      "Epoch: 2590\n",
      "Total Loss: 0.16052226722240448\n",
      "----------------------------------\n",
      "Epoch: 2600\n",
      "Total Loss: 0.057124506682157516\n",
      "----------------------------------\n",
      "Epoch: 2610\n",
      "Total Loss: 0.09322534501552582\n",
      "----------------------------------\n",
      "Epoch: 2620\n",
      "Total Loss: 0.08026416599750519\n",
      "----------------------------------\n",
      "Epoch: 2630\n",
      "Total Loss: 0.04334057494997978\n",
      "----------------------------------\n",
      "Epoch: 2640\n",
      "Total Loss: 0.0876808762550354\n",
      "----------------------------------\n",
      "Epoch: 2650\n",
      "Total Loss: 0.059305477887392044\n",
      "----------------------------------\n",
      "Epoch: 2660\n",
      "Total Loss: 0.06464994698762894\n",
      "----------------------------------\n",
      "Epoch: 2670\n",
      "Total Loss: 0.11978983134031296\n",
      "----------------------------------\n",
      "Epoch: 2680\n",
      "Total Loss: 0.09242726117372513\n",
      "----------------------------------\n",
      "Epoch: 2690\n",
      "Total Loss: 0.06812074035406113\n",
      "----------------------------------\n",
      "Epoch: 2700\n",
      "Total Loss: 0.12826627492904663\n",
      "----------------------------------\n",
      "Epoch: 2710\n",
      "Total Loss: 0.09409288316965103\n",
      "----------------------------------\n",
      "Epoch: 2720\n",
      "Total Loss: 0.07412858307361603\n",
      "----------------------------------\n",
      "Epoch: 2730\n",
      "Total Loss: 0.0349615179002285\n",
      "----------------------------------\n",
      "Epoch: 2740\n",
      "Total Loss: 0.13166646659374237\n",
      "----------------------------------\n",
      "Epoch: 2750\n",
      "Total Loss: 0.04253195598721504\n",
      "----------------------------------\n",
      "Epoch: 2760\n",
      "Total Loss: 0.05994657427072525\n",
      "----------------------------------\n",
      "Epoch: 2770\n",
      "Total Loss: 0.08099562674760818\n",
      "----------------------------------\n",
      "Epoch: 2780\n",
      "Total Loss: 0.07843655347824097\n",
      "----------------------------------\n",
      "Epoch: 2790\n",
      "Total Loss: 0.07122360169887543\n",
      "----------------------------------\n",
      "Epoch: 2800\n",
      "Total Loss: 0.05348096042871475\n",
      "----------------------------------\n",
      "Epoch: 2810\n",
      "Total Loss: 0.07797233015298843\n",
      "----------------------------------\n",
      "Epoch: 2820\n",
      "Total Loss: 0.0423714742064476\n",
      "----------------------------------\n",
      "Epoch: 2830\n",
      "Total Loss: 0.11786451935768127\n",
      "----------------------------------\n",
      "Epoch: 2840\n",
      "Total Loss: 0.06538929045200348\n",
      "----------------------------------\n",
      "Epoch: 2850\n",
      "Total Loss: 0.0575362928211689\n",
      "----------------------------------\n",
      "Epoch: 2860\n",
      "Total Loss: 0.1711128205060959\n",
      "----------------------------------\n",
      "Epoch: 2870\n",
      "Total Loss: 0.14398697018623352\n",
      "----------------------------------\n",
      "Epoch: 2880\n",
      "Total Loss: 0.08687824010848999\n",
      "----------------------------------\n",
      "Epoch: 2890\n",
      "Total Loss: 0.03819809854030609\n",
      "----------------------------------\n",
      "Epoch: 2900\n",
      "Total Loss: 0.05212577432394028\n",
      "----------------------------------\n",
      "Epoch: 2910\n",
      "Total Loss: 0.05002430081367493\n",
      "----------------------------------\n",
      "Epoch: 2920\n",
      "Total Loss: 0.060596734285354614\n",
      "----------------------------------\n",
      "Epoch: 2930\n",
      "Total Loss: 0.09803977608680725\n",
      "----------------------------------\n",
      "Epoch: 2940\n",
      "Total Loss: 0.18380217254161835\n",
      "----------------------------------\n",
      "Epoch: 2950\n",
      "Total Loss: 0.09168219566345215\n",
      "----------------------------------\n",
      "Epoch: 2960\n",
      "Total Loss: 0.09473098814487457\n",
      "----------------------------------\n",
      "Epoch: 2970\n",
      "Total Loss: 0.048573240637779236\n",
      "----------------------------------\n",
      "Epoch: 2980\n",
      "Total Loss: 0.09774484485387802\n",
      "----------------------------------\n",
      "Epoch: 2990\n",
      "Total Loss: 0.06122909113764763\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3000\n",
    "best_model, loss_, epochs = train_loop(model, n_epochs, data_loader.data, data_loader.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, loss_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_seq, max_length=5, SOS_token=5, EOS_token=6, verbose=False):\n",
    "    model.eval()\n",
    "    \n",
    "    target_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
    "\n",
    "    # Asks model to give only one item, the next thing it thinks is most probable \n",
    "    # to continue the sentence\n",
    "    for i in range(max_length):\n",
    "        sequence_length = target_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "        src_mask = model.get_tgt_mask(input_seq.size(1)).to(device)\n",
    "        \n",
    "        #print(input_seq)\n",
    "        pred = model(input_seq, target_input, src_mask, tgt_mask)\n",
    "        \n",
    "        pred = pred.permute(2, 0, 1)\n",
    "        \n",
    "        if verbose:\n",
    "            print(pred.cpu().squeeze().detach().numpy())\n",
    "            print(pred.topk(1))\n",
    "            print(pred.topk(1)[1].view(-1)[-1].item())\n",
    "        \n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability, twas view(-1)[-1] before\n",
    "    \n",
    "        next_item = torch.tensor([[next_item]], device=device)\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        target_input = torch.cat((target_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "        elif (len(target_input.view(-1)) >= len(input_seq[0])):\n",
    "            break\n",
    "\n",
    "    return target_input.view(-1).tolist()\n",
    "\n",
    "def test_example(model, example, data_loader, i):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]} --> Prediction: {result[1:-1]}  vs Actual: {data_loader.target[i].view(-1).tolist()[1:-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moment of Truth; Has it at least memorized the data?\n",
      "------------------------------------\n",
      "Input: [0, 0, 0] --> Prediction: [0, 0, 0]  vs Actual: [0, 0, 0]\n",
      "\n",
      "Input: [0, 0, 1] --> Prediction: [1, 1, 0]  vs Actual: [1, 1, 0]\n",
      "\n",
      "Input: [0, 0, 2] --> Prediction: [2, 2, 0]  vs Actual: [2, 2, 0]\n",
      "\n",
      "Input: [0, 1, 1] --> Prediction: [2, 1, 1]  vs Actual: [2, 1, 1]\n",
      "\n",
      "Input: [0, 1, 2] --> Prediction: [3, 2, 1]  vs Actual: [3, 2, 1]\n",
      "\n",
      "Input: [0, 2, 0] --> Prediction: [2, 0, 2]  vs Actual: [2, 0, 2]\n",
      "\n",
      "Input: [0, 2, 1] --> Prediction: [3, 1, 2]  vs Actual: [3, 1, 2]\n",
      "\n",
      "Input: [0, 2, 2] --> Prediction: [4, 2, 2]  vs Actual: [4, 2, 2]\n",
      "\n",
      "Input: [1, 0, 0] --> Prediction: [0, 1, 1]  vs Actual: [0, 1, 1]\n",
      "\n",
      "Input: [1, 0, 1] --> Prediction: [1, 2, 1]  vs Actual: [1, 2, 1]\n",
      "\n",
      "Input: [1, 0, 2] --> Prediction: [2, 3, 1]  vs Actual: [2, 3, 1]\n",
      "\n",
      "Input: [1, 1, 0] --> Prediction: [1, 1, 2]  vs Actual: [1, 1, 2]\n",
      "\n",
      "Input: [1, 1, 1] --> Prediction: [2, 2, 2]  vs Actual: [2, 2, 2]\n",
      "\n",
      "Input: [1, 1, 2] --> Prediction: [3, 3, 2]  vs Actual: [3, 3, 2]\n",
      "\n",
      "Input: [1, 2, 1] --> Prediction: [3, 2, 3]  vs Actual: [3, 2, 3]\n",
      "\n",
      "Input: [1, 2, 2] --> Prediction: [4, 3, 3]  vs Actual: [4, 3, 3]\n",
      "\n",
      "Input: [2, 0, 0] --> Prediction: [0, 2, 2]  vs Actual: [0, 2, 2]\n",
      "\n",
      "Input: [2, 0, 1] --> Prediction: [1, 3, 2]  vs Actual: [1, 3, 2]\n",
      "\n",
      "Input: [2, 0, 2] --> Prediction: [2, 4, 2]  vs Actual: [2, 4, 2]\n",
      "\n",
      "Input: [2, 1, 0] --> Prediction: [1, 2, 3]  vs Actual: [1, 2, 3]\n",
      "\n",
      "Input: [2, 1, 1] --> Prediction: [2, 3, 3]  vs Actual: [2, 3, 3]\n",
      "\n",
      "Input: [2, 1, 2] --> Prediction: [3, 4, 3]  vs Actual: [3, 4, 3]\n",
      "\n",
      "Input: [2, 2, 0] --> Prediction: [2, 2, 4]  vs Actual: [2, 2, 4]\n",
      "\n",
      "Input: [2, 2, 1] --> Prediction: [3, 3, 4]  vs Actual: [3, 3, 4]\n",
      "\n",
      "Input: [2, 2, 2] --> Prediction: [4, 4, 4]  vs Actual: [4, 4, 4]\n",
      "\n",
      "Did it maybe possible work? Ich würde viele Aufregung haben, als das passieren würde!\n",
      "Let uns check the Wahrscheinlichkeiten, yo: \n",
      "[-3.6632967  -6.18621     1.9015007   1.1325839   8.783852    0.47938222\n",
      " -3.444592  ]\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[8.7839]]], device='cuda:0', grad_fn=<TopkBackward>),\n",
      "indices=tensor([[[4]]], device='cuda:0'))\n",
      "4\n",
      "[[-3.6632965  -6.186209    1.9015012   1.1325835   8.783852    0.47938225\n",
      "  -3.4445925 ]\n",
      " [-3.696113   -7.683422    0.91501486  2.046206    8.855333    0.43034053\n",
      "  -0.56632787]]\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[8.7839]],\n",
      "\n",
      "        [[8.8553]]], device='cuda:0', grad_fn=<TopkBackward>),\n",
      "indices=tensor([[[4]],\n",
      "\n",
      "        [[4]]], device='cuda:0'))\n",
      "4\n",
      "[[-3.6632965  -6.186209    1.9015012   1.1325835   8.783852    0.47938225\n",
      "  -3.4445925 ]\n",
      " [-3.696113   -7.683422    0.91501474  2.046206    8.855333    0.43034047\n",
      "  -0.5663286 ]\n",
      " [-3.3948264  -8.461847   -0.277708    1.3562165   7.4476113   0.18772475\n",
      "   4.025031  ]]\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[8.7839]],\n",
      "\n",
      "        [[8.8553]],\n",
      "\n",
      "        [[7.4476]]], device='cuda:0', grad_fn=<TopkBackward>),\n",
      "indices=tensor([[[4]],\n",
      "\n",
      "        [[4]],\n",
      "\n",
      "        [[4]]], device='cuda:0'))\n",
      "4\n",
      "[[-3.6632965  -6.186209    1.9015012   1.1325835   8.783852    0.47938225\n",
      "  -3.4445925 ]\n",
      " [-3.696113   -7.683422    0.91501474  2.046206    8.855333    0.43034047\n",
      "  -0.5663286 ]\n",
      " [-3.3948264  -8.461847   -0.277708    1.3562165   7.4476113   0.18772475\n",
      "   4.025031  ]\n",
      " [-2.9272215  -7.714295   -1.0833716   0.51261073  4.8899193  -0.14502516\n",
      "   7.4342504 ]]\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[8.7839]],\n",
      "\n",
      "        [[8.8553]],\n",
      "\n",
      "        [[7.4476]],\n",
      "\n",
      "        [[7.4343]]], device='cuda:0', grad_fn=<TopkBackward>),\n",
      "indices=tensor([[[4]],\n",
      "\n",
      "        [[4]],\n",
      "\n",
      "        [[4]],\n",
      "\n",
      "        [[6]]], device='cuda:0'))\n",
      "6\n",
      "Example\n",
      "Input: [2, 2, 2] --> Continutation: [4, 4, 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The reckoning\n",
    "print('Moment of Truth; Has it at least memorized the data?')\n",
    "print('------------------------------------')\n",
    "\n",
    "for i, sequence in enumerate(data_loader.data):\n",
    "    sequence = torch.unsqueeze(sequence, dim=0)\n",
    "    test_example(best_model, sequence, data_loader, i)\n",
    "print('Did it maybe possible work? Ich würde viele Aufregung haben, als das passieren würde!')\n",
    "\n",
    "# This time, I removed 0, 1, 0 from the dataset and added 2, 2, 2 to the set\n",
    "print('Let uns check the Wahrscheinlichkeiten, yo: ')\n",
    "example = torch.tensor([[5, 2, 2, 2, 6]]).type(torch.long).to(device)\n",
    "result = predict(model, example, verbose=True)\n",
    "print(f\"Example\")\n",
    "print(f\"Input: {example.view(-1).tolist()[1:-1]} --> Continutation: {result[1:-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 60])\n",
      "tensor([-0.0249, -2.8021, -0.4377,  0.9583,  0.3198,  0.9471,  1.8276,  0.2293,\n",
      "         1.3250, -0.4521,  1.3166,  1.0119,  0.9855, -1.0482, -0.7652,  0.6970,\n",
      "         0.0161,  0.2890,  1.1278,  0.0309, -0.6450, -0.0514, -0.6065, -0.9043,\n",
      "         0.2349,  0.1062, -1.1840,  1.3056,  0.6100, -1.0321, -0.3453, -0.9089,\n",
      "        -0.9279,  0.2735, -1.2436, -2.4410, -0.0768,  0.2881,  0.2943,  2.1848,\n",
      "         0.0882, -0.1254, -0.7018,  0.0851, -0.5261, -2.7904, -0.1752, -1.1616,\n",
      "        -0.4065, -1.5997,  0.4245, -1.2996, -1.6391,  0.1359, -0.8933, -2.1602,\n",
      "        -2.9080, -1.5079, -0.6556,  0.4100], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([[-0.0249, -2.8021, -0.4377,  0.9583,  0.3198,  0.9471,  1.8276,  0.2293,\n",
      "          1.3250, -0.4521,  1.3166,  1.0119,  0.9855, -1.0482, -0.7652,  0.6970,\n",
      "          0.0161,  0.2890,  1.1278,  0.0309, -0.6450, -0.0514, -0.6065, -0.9043,\n",
      "          0.2349,  0.1062, -1.1840,  1.3056,  0.6100, -1.0321, -0.3453, -0.9089,\n",
      "         -0.9279,  0.2735, -1.2436, -2.4410, -0.0768,  0.2881,  0.2943,  2.1848,\n",
      "          0.0882, -0.1254, -0.7018,  0.0851, -0.5261, -2.7904, -0.1752, -1.1616,\n",
      "         -0.4065, -1.5997,  0.4245, -1.2996, -1.6391,  0.1359, -0.8933, -2.1602,\n",
      "         -2.9080, -1.5079, -0.6556,  0.4100]], device='cuda:0',\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(best_model.embedding.weight.size())\n",
    "print(best_model.embedding.weight[0])\n",
    "print(best_model.embedding(torch.tensor([0]).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
