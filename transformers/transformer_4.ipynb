{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' To make an end is to make a beginning\n",
    "\n",
    "\n",
    "    b a t c h  g r a d i e n t  d e s c e n t \n",
    "    \n",
    "    Play around with number of heads and number of layers; what is the minimum optimal number\n",
    "    \n",
    "    Give it unseen dataset within allowable set (i.e. give 0, 1, 0 instead of 2, 2, 2)\n",
    "    Go from 0-99 :)\n",
    "    Play around with model, embedding dimension\n",
    "    \n",
    "    Another test: keep 0-2, make length longer -->? random\n",
    "    Make random data set, much larger\n",
    "    \n",
    "    Does it learn length?\n",
    "        Can you feed it varying input lengths with corresponding output lengths and then learn how to do it itself  \n",
    "        \n",
    "    For actual physics: make embedding --> nn.Linear\n",
    "        Would this linear be an actual neural network\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import copy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataLoader():\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.target = []\n",
    "        self.sz = 0\n",
    "        \n",
    "    def read_data(self, file):\n",
    "        with open(file) as data_file:\n",
    "            data_reader = csv.reader(data_file)\n",
    "            for row in data_reader:\n",
    "                sequence = [int(row[0]), int(row[1]), int(row[2])]\n",
    "                self.data.append(sequence)\n",
    "        self.data = np.array(self.data)\n",
    "        \n",
    "    def get_target(self):\n",
    "        self.target = np.empty(self.data.shape)\n",
    "        self.target[:, 0] = self.data[:, 1] + self.data[:, 2]\n",
    "        self.target[:, 1] = self.data[:, 0] + self.data[:, 2]\n",
    "        self.target[:, 2] = self.data[:, 0] + self.data[:, 1]\n",
    "    \n",
    "    def tensorize(self):\n",
    "        SOS_token = 5\n",
    "        EOS_token = 6\n",
    "        self.data = torch.tensor(self.data).to(torch.long).to(device)\n",
    "        self.target = torch.tensor(self.target).to(torch.long).to(device)\n",
    "            \n",
    "        SOS_ = (torch.ones((self.data.shape[0], 1)) * SOS_token).to(torch.long).to(device)\n",
    "        EOS_ = (torch.ones((self.data.shape[0], 1)) * EOS_token).to(torch.long).to(device)\n",
    "        \n",
    "        self.data = torch.cat((SOS_, self.data, EOS_), 1).to(device)\n",
    "        self.target = torch.cat((SOS_, self.target, EOS_), 1).to(device)\n",
    "        \n",
    "    def sanity_check(self, index, verbose=True):\n",
    "        print(f'Data has shape of {self.data.shape}')\n",
    "        print(f'Target has shape of {self.target.shape}')\n",
    "        print(f'Compare data and target at index {index}: ')\n",
    "        print(f'    data[{index}] = {self.data[index]}')\n",
    "        print(f'    target[{index}] = {self.target[index]}')\n",
    "        \n",
    "        if verbose:\n",
    "            for index in range(1, len(self.data)+1):\n",
    "                print(f'I: {np.array(self.data[index-1].cpu())} --> O: {np.array(self.target[index-1].cpu())}  \\*/  ', end='')\n",
    "                if index != 0 and index % 2 == 0:\n",
    "                    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has shape of torch.Size([25, 5])\n",
      "Target has shape of torch.Size([25, 5])\n",
      "Compare data and target at index 2: \n",
      "    data[2] = tensor([5, 0, 0, 2, 6])\n",
      "    target[2] = tensor([5, 2, 2, 0, 6])\n",
      "I: [5 0 0 0 6] --> O: [5 0 0 0 6]  \\*/  I: [5 0 0 1 6] --> O: [5 1 1 0 6]  \\*/  \n",
      "\n",
      "I: [5 0 0 2 6] --> O: [5 2 2 0 6]  \\*/  I: [5 0 1 1 6] --> O: [5 2 1 1 6]  \\*/  \n",
      "\n",
      "I: [5 0 1 2 6] --> O: [5 3 2 1 6]  \\*/  I: [5 0 2 0 6] --> O: [5 2 0 2 6]  \\*/  \n",
      "\n",
      "I: [5 0 2 1 6] --> O: [5 3 1 2 6]  \\*/  I: [5 0 2 2 6] --> O: [5 4 2 2 6]  \\*/  \n",
      "\n",
      "I: [5 1 0 0 6] --> O: [5 0 1 1 6]  \\*/  I: [5 1 0 1 6] --> O: [5 1 2 1 6]  \\*/  \n",
      "\n",
      "I: [5 1 0 2 6] --> O: [5 2 3 1 6]  \\*/  I: [5 1 1 0 6] --> O: [5 1 1 2 6]  \\*/  \n",
      "\n",
      "I: [5 1 1 1 6] --> O: [5 2 2 2 6]  \\*/  I: [5 1 1 2 6] --> O: [5 3 3 2 6]  \\*/  \n",
      "\n",
      "I: [5 1 2 1 6] --> O: [5 3 2 3 6]  \\*/  I: [5 1 2 2 6] --> O: [5 4 3 3 6]  \\*/  \n",
      "\n",
      "I: [5 2 0 0 6] --> O: [5 0 2 2 6]  \\*/  I: [5 2 0 1 6] --> O: [5 1 3 2 6]  \\*/  \n",
      "\n",
      "I: [5 2 0 2 6] --> O: [5 2 4 2 6]  \\*/  I: [5 2 1 0 6] --> O: [5 1 2 3 6]  \\*/  \n",
      "\n",
      "I: [5 2 1 1 6] --> O: [5 2 3 3 6]  \\*/  I: [5 2 1 2 6] --> O: [5 3 4 3 6]  \\*/  \n",
      "\n",
      "I: [5 2 2 0 6] --> O: [5 2 2 4 6]  \\*/  I: [5 2 2 1 6] --> O: [5 3 3 4 6]  \\*/  \n",
      "\n",
      "I: [5 2 2 2 6] --> O: [5 4 4 4 6]  \\*/  "
     ]
    }
   ],
   "source": [
    "data_loader = dataLoader()\n",
    "data_loader.read_data('data.csv')\n",
    "data_loader.get_target()\n",
    "data_loader.tensorize()\n",
    "data_loader.sanity_check(2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "\n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        global pos_encoding #I know it's bad practice to make it global, this is just a test for visualizing attention \n",
    "        pos_encoding = torch.zeros(max_len, d_model) \n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.long).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, d_model, 2).long() * (-math.log(10000.0)) / d_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/d_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/d_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_token, d_model, n_head, n_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            d_model = d_model, \n",
    "            dropout_p = 0.1,\n",
    "            max_len = 100)\n",
    "        self.embedding = nn.Embedding(n_token, d_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers=n_layers,\n",
    "            num_decoder_layers=n_layers)\n",
    "        \n",
    "        # For floating point with one output, then self.out = nn.Linear(d_model, 1)\n",
    "        self.out = nn.Linear(d_model, n_token) # Learned linear at the end where output of decoder is run through\n",
    "        \n",
    "    def get_tgt_mask(self, size):\n",
    "    \n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \n",
    "        src = (self.embedding(src) * math.sqrt(self.d_model)).to(device)\n",
    "        tgt = (self.embedding(tgt) * math.sqrt(self.d_model)).to(device)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "        \n",
    "        src = src.permute(1, 0, 2)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        \n",
    "        transformer_output = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask).to(device)\n",
    "        output = self.out(transformer_output).to(device)\n",
    "        \n",
    "        output = output.permute(1, 2, 0).to(device)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens are 7, six categories to calculate probability distributions over (i.e. 0, 1, 2, 3, 4 + sos + eos)\n",
      "src:\n",
      "  Size: torch.Size([2, 3])\n",
      "  Tensor: tensor([[0, 0, 1],\n",
      "        [1, 0, 1]])\n",
      "tgt:\n",
      "  Size: torch.Size([2, 3])\n",
      "  Tensor: tensor([[0, 0, 1],\n",
      "        [1, 0, 1]])\n",
      "output:\n",
      "  Size: torch.Size([2, 7, 3])\n",
      "  Tensor: tensor([[[-0.1458, -0.2840,  0.2615],\n",
      "         [-0.2703, -0.2861, -0.4444],\n",
      "         [ 0.6795,  1.0740,  1.2760],\n",
      "         [-0.1887,  0.2154, -0.1580],\n",
      "         [-0.2072, -0.1858, -0.4202],\n",
      "         [ 0.3438,  0.1502,  0.3404],\n",
      "         [-0.4700, -0.5634, -0.4874]],\n",
      "\n",
      "        [[ 1.1463,  0.4543,  0.7416],\n",
      "         [-0.4945, -0.5204, -0.1119],\n",
      "         [-0.4055, -0.0192,  0.6722],\n",
      "         [ 0.1306, -0.0818,  0.4089],\n",
      "         [-0.4134, -0.1105, -0.1334],\n",
      "         [ 0.2617,  0.3863,  0.4047],\n",
      "         [-0.0519, -0.3387, -0.0245]]], grad_fn=<PermuteBackward0>)\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "src:\n",
      "  Size: torch.Size([2, 3])\n",
      "  Tensor: tensor([[1, 1, 0],\n",
      "        [0, 1, 0]])\n",
      "tgt_input:\n",
      "  Size: torch.Size([2, 2])\n",
      "  Tensor: tensor([[1, 1],\n",
      "        [0, 1]])\n",
      "output:\n",
      "  Size: torch.Size([2, 7, 2])\n",
      "  Tensor: tensor([[[ 0.4424,  0.4636],\n",
      "         [-0.2823, -0.3847],\n",
      "         [ 0.3768,  0.7642],\n",
      "         [-0.5881, -0.7564],\n",
      "         [-0.1105,  0.1010],\n",
      "         [ 0.1079,  0.2899],\n",
      "         [-0.7192, -0.7666]],\n",
      "\n",
      "        [[-0.1779,  0.6871],\n",
      "         [-0.1580,  0.4858],\n",
      "         [ 0.3023,  0.1679],\n",
      "         [-0.5489,  0.3571],\n",
      "         [-0.6161, -1.0473],\n",
      "         [ 0.3198, -0.2828],\n",
      "         [-0.4011, -0.1559]]], grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's define an instance :)\n",
    "d_model = 60 # Let's try embedding brotha\n",
    "n_token = 7 # Only do 0, 1, 2?\n",
    "n_head = 3 # Remember that d_model=d_embedding must be divisible by number of n_head\n",
    "n_layers = 4\n",
    "\n",
    "model = TransformerModel(n_token, d_model, n_head, n_layers).to(device)\n",
    "\n",
    "print(f'The number of tokens are {n_token}, six categories to calculate probability distributions over (i.e. 0, 1, 2, 3, 4 + sos + eos)')\n",
    "\n",
    "# Check da dimension\n",
    "src = torch.randint(size=(2, 3), low=0, high=2).to(device)\n",
    "tgt = src.to(device)\n",
    "\n",
    "output = model(src, src).to(device)\n",
    "print(f'src:\\n  Size: {src.size()}\\n  Tensor: {src}')\n",
    "print(f'tgt:\\n  Size: {tgt.size()}\\n  Tensor: {tgt}')\n",
    "print(f'output:\\n  Size: {output.size()}\\n  Tensor: {output}\\n')\n",
    "\n",
    "src = torch.randint(size=(2, 3), low=0, high=2).to(device)\n",
    "tgt = src.to(device)\n",
    "tgt_input = tgt[:, :-1].to(device)\n",
    "tgt_expected = tgt[:, 1:].to(device)\n",
    "output = model(src, tgt_input).to(device)\n",
    "\n",
    "print('----------------------------------------------------------------------------')\n",
    "print(f'src:\\n  Size: {src.size()}\\n  Tensor: {src}')\n",
    "print(f'tgt_input:\\n  Size: {tgt_input.size()}\\n  Tensor: {tgt_input}')\n",
    "print(f'output:\\n  Size: {output.size()}\\n  Tensor: {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# America needs more trains\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, data, target):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    data = data.type(torch.long).to(device)\n",
    "    target = target.type(torch.long).to(device)\n",
    "          \n",
    "    target_in = target[:, :-1]\n",
    "    target_expected = target[:, 1:]\n",
    "\n",
    "    sequence_length = target_in.size(1)\n",
    "    \n",
    "    tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "    src_mask = model.get_tgt_mask(data.size(1)).to(device)\n",
    "    pred = model(data, target_in, src_mask, tgt_mask)\n",
    "    \n",
    "    loss = (loss_fn(pred, target_expected)).type(torch.float)\n",
    "        \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss\n",
    "    \n",
    "def train_loop(model, n_epochs, data, target):\n",
    "    best_loss = 10000.0\n",
    "    best_model = copy.deepcopy(model).to(device)\n",
    "    loss_ = np.array([])\n",
    "    epochs = np.array([])\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        loss = train(model, data, target)\n",
    "        loss_ = np.append(loss_, loss)\n",
    "        epochs = np.append(epochs, i)\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = copy.deepcopy(model).to(device)\n",
    "                     \n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {i}\\nTotal Loss: {loss}')\n",
    "            print(f'----------------------------------')\n",
    "    \n",
    "    return best_model, loss_, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Total Loss: 2.120666742324829\n",
      "----------------------------------\n",
      "Epoch: 10\n",
      "Total Loss: 1.8274704217910767\n",
      "----------------------------------\n",
      "Epoch: 20\n",
      "Total Loss: 1.6440801620483398\n",
      "----------------------------------\n",
      "Epoch: 30\n",
      "Total Loss: 1.5627658367156982\n",
      "----------------------------------\n",
      "Epoch: 40\n",
      "Total Loss: 1.4884748458862305\n",
      "----------------------------------\n",
      "Epoch: 50\n",
      "Total Loss: 1.4251258373260498\n",
      "----------------------------------\n",
      "Epoch: 60\n",
      "Total Loss: 1.345024824142456\n",
      "----------------------------------\n",
      "Epoch: 70\n",
      "Total Loss: 1.2551624774932861\n",
      "----------------------------------\n",
      "Epoch: 80\n",
      "Total Loss: 1.2125378847122192\n",
      "----------------------------------\n",
      "Epoch: 90\n",
      "Total Loss: 1.2092173099517822\n",
      "----------------------------------\n",
      "Epoch: 100\n",
      "Total Loss: 1.1106821298599243\n",
      "----------------------------------\n",
      "Epoch: 110\n",
      "Total Loss: 1.066449761390686\n",
      "----------------------------------\n",
      "Epoch: 120\n",
      "Total Loss: 1.0382733345031738\n",
      "----------------------------------\n",
      "Epoch: 130\n",
      "Total Loss: 1.0848180055618286\n",
      "----------------------------------\n",
      "Epoch: 140\n",
      "Total Loss: 1.0236550569534302\n",
      "----------------------------------\n",
      "Epoch: 150\n",
      "Total Loss: 0.9642364978790283\n",
      "----------------------------------\n",
      "Epoch: 160\n",
      "Total Loss: 1.0001929998397827\n",
      "----------------------------------\n",
      "Epoch: 170\n",
      "Total Loss: 0.8919957876205444\n",
      "----------------------------------\n",
      "Epoch: 180\n",
      "Total Loss: 0.9114036560058594\n",
      "----------------------------------\n",
      "Epoch: 190\n",
      "Total Loss: 0.8970615267753601\n",
      "----------------------------------\n",
      "Epoch: 200\n",
      "Total Loss: 0.7856255173683167\n",
      "----------------------------------\n",
      "Epoch: 210\n",
      "Total Loss: 0.8392089009284973\n",
      "----------------------------------\n",
      "Epoch: 220\n",
      "Total Loss: 0.860005795955658\n",
      "----------------------------------\n",
      "Epoch: 230\n",
      "Total Loss: 0.7504138946533203\n",
      "----------------------------------\n",
      "Epoch: 240\n",
      "Total Loss: 0.7117164134979248\n",
      "----------------------------------\n",
      "Epoch: 250\n",
      "Total Loss: 0.7048246264457703\n",
      "----------------------------------\n",
      "Epoch: 260\n",
      "Total Loss: 0.6849427819252014\n",
      "----------------------------------\n",
      "Epoch: 270\n",
      "Total Loss: 0.6862006187438965\n",
      "----------------------------------\n",
      "Epoch: 280\n",
      "Total Loss: 0.7102837562561035\n",
      "----------------------------------\n",
      "Epoch: 290\n",
      "Total Loss: 0.6600069403648376\n",
      "----------------------------------\n",
      "Epoch: 300\n",
      "Total Loss: 0.640439510345459\n",
      "----------------------------------\n",
      "Epoch: 310\n",
      "Total Loss: 0.6226333379745483\n",
      "----------------------------------\n",
      "Epoch: 320\n",
      "Total Loss: 0.5641638040542603\n",
      "----------------------------------\n",
      "Epoch: 330\n",
      "Total Loss: 0.5543386936187744\n",
      "----------------------------------\n",
      "Epoch: 340\n",
      "Total Loss: 0.6137489080429077\n",
      "----------------------------------\n",
      "Epoch: 350\n",
      "Total Loss: 0.6057146191596985\n",
      "----------------------------------\n",
      "Epoch: 360\n",
      "Total Loss: 0.5777698755264282\n",
      "----------------------------------\n",
      "Epoch: 370\n",
      "Total Loss: 0.5762130618095398\n",
      "----------------------------------\n",
      "Epoch: 380\n",
      "Total Loss: 0.5201908349990845\n",
      "----------------------------------\n",
      "Epoch: 390\n",
      "Total Loss: 0.6496922969818115\n",
      "----------------------------------\n",
      "Epoch: 400\n",
      "Total Loss: 0.499297559261322\n",
      "----------------------------------\n",
      "Epoch: 410\n",
      "Total Loss: 0.4872455596923828\n",
      "----------------------------------\n",
      "Epoch: 420\n",
      "Total Loss: 0.4903874695301056\n",
      "----------------------------------\n",
      "Epoch: 430\n",
      "Total Loss: 0.4003020226955414\n",
      "----------------------------------\n",
      "Epoch: 440\n",
      "Total Loss: 0.4541870057582855\n",
      "----------------------------------\n",
      "Epoch: 450\n",
      "Total Loss: 0.4633266031742096\n",
      "----------------------------------\n",
      "Epoch: 460\n",
      "Total Loss: 0.4431701600551605\n",
      "----------------------------------\n",
      "Epoch: 470\n",
      "Total Loss: 0.4394628405570984\n",
      "----------------------------------\n",
      "Epoch: 480\n",
      "Total Loss: 0.48623648285865784\n",
      "----------------------------------\n",
      "Epoch: 490\n",
      "Total Loss: 0.5453697443008423\n",
      "----------------------------------\n",
      "Epoch: 500\n",
      "Total Loss: 0.40184155106544495\n",
      "----------------------------------\n",
      "Epoch: 510\n",
      "Total Loss: 0.4919798672199249\n",
      "----------------------------------\n",
      "Epoch: 520\n",
      "Total Loss: 0.40497204661369324\n",
      "----------------------------------\n",
      "Epoch: 530\n",
      "Total Loss: 0.37002864480018616\n",
      "----------------------------------\n",
      "Epoch: 540\n",
      "Total Loss: 0.3419853448867798\n",
      "----------------------------------\n",
      "Epoch: 550\n",
      "Total Loss: 0.40541285276412964\n",
      "----------------------------------\n",
      "Epoch: 560\n",
      "Total Loss: 0.398502916097641\n",
      "----------------------------------\n",
      "Epoch: 570\n",
      "Total Loss: 0.4370170533657074\n",
      "----------------------------------\n",
      "Epoch: 580\n",
      "Total Loss: 0.3510063588619232\n",
      "----------------------------------\n",
      "Epoch: 590\n",
      "Total Loss: 0.3564552068710327\n",
      "----------------------------------\n",
      "Epoch: 600\n",
      "Total Loss: 0.3926449716091156\n",
      "----------------------------------\n",
      "Epoch: 610\n",
      "Total Loss: 0.2826196253299713\n",
      "----------------------------------\n",
      "Epoch: 620\n",
      "Total Loss: 0.28505635261535645\n",
      "----------------------------------\n",
      "Epoch: 630\n",
      "Total Loss: 0.41694119572639465\n",
      "----------------------------------\n",
      "Epoch: 640\n",
      "Total Loss: 0.42371436953544617\n",
      "----------------------------------\n",
      "Epoch: 650\n",
      "Total Loss: 0.36776724457740784\n",
      "----------------------------------\n",
      "Epoch: 660\n",
      "Total Loss: 0.28299781680107117\n",
      "----------------------------------\n",
      "Epoch: 670\n",
      "Total Loss: 0.33389243483543396\n",
      "----------------------------------\n",
      "Epoch: 680\n",
      "Total Loss: 0.3671240210533142\n",
      "----------------------------------\n",
      "Epoch: 690\n",
      "Total Loss: 0.3014097809791565\n",
      "----------------------------------\n",
      "Epoch: 700\n",
      "Total Loss: 0.3967175781726837\n",
      "----------------------------------\n",
      "Epoch: 710\n",
      "Total Loss: 0.2859596312046051\n",
      "----------------------------------\n",
      "Epoch: 720\n",
      "Total Loss: 0.32272690534591675\n",
      "----------------------------------\n",
      "Epoch: 730\n",
      "Total Loss: 0.29810264706611633\n",
      "----------------------------------\n",
      "Epoch: 740\n",
      "Total Loss: 0.3571876883506775\n",
      "----------------------------------\n",
      "Epoch: 750\n",
      "Total Loss: 0.2865177392959595\n",
      "----------------------------------\n",
      "Epoch: 760\n",
      "Total Loss: 0.3013194799423218\n",
      "----------------------------------\n",
      "Epoch: 770\n",
      "Total Loss: 0.35087111592292786\n",
      "----------------------------------\n",
      "Epoch: 780\n",
      "Total Loss: 0.21849876642227173\n",
      "----------------------------------\n",
      "Epoch: 790\n",
      "Total Loss: 0.35939788818359375\n",
      "----------------------------------\n",
      "Epoch: 800\n",
      "Total Loss: 0.2770136594772339\n",
      "----------------------------------\n",
      "Epoch: 810\n",
      "Total Loss: 0.2617960274219513\n",
      "----------------------------------\n",
      "Epoch: 820\n",
      "Total Loss: 0.3756072521209717\n",
      "----------------------------------\n",
      "Epoch: 830\n",
      "Total Loss: 0.2848677933216095\n",
      "----------------------------------\n",
      "Epoch: 840\n",
      "Total Loss: 0.23273658752441406\n",
      "----------------------------------\n",
      "Epoch: 850\n",
      "Total Loss: 0.3163403570652008\n",
      "----------------------------------\n",
      "Epoch: 860\n",
      "Total Loss: 0.27605053782463074\n",
      "----------------------------------\n",
      "Epoch: 870\n",
      "Total Loss: 0.2588582932949066\n",
      "----------------------------------\n",
      "Epoch: 880\n",
      "Total Loss: 0.2559124827384949\n",
      "----------------------------------\n",
      "Epoch: 890\n",
      "Total Loss: 0.1928064376115799\n",
      "----------------------------------\n",
      "Epoch: 900\n",
      "Total Loss: 0.23706252872943878\n",
      "----------------------------------\n",
      "Epoch: 910\n",
      "Total Loss: 0.20859113335609436\n",
      "----------------------------------\n",
      "Epoch: 920\n",
      "Total Loss: 0.22123736143112183\n",
      "----------------------------------\n",
      "Epoch: 930\n",
      "Total Loss: 0.3173118233680725\n",
      "----------------------------------\n",
      "Epoch: 940\n",
      "Total Loss: 0.2513163685798645\n",
      "----------------------------------\n",
      "Epoch: 950\n",
      "Total Loss: 0.2676010727882385\n",
      "----------------------------------\n",
      "Epoch: 960\n",
      "Total Loss: 0.24946463108062744\n",
      "----------------------------------\n",
      "Epoch: 970\n",
      "Total Loss: 0.18212908506393433\n",
      "----------------------------------\n",
      "Epoch: 980\n",
      "Total Loss: 0.21245574951171875\n",
      "----------------------------------\n",
      "Epoch: 990\n",
      "Total Loss: 0.22004428505897522\n",
      "----------------------------------\n",
      "Epoch: 1000\n",
      "Total Loss: 0.2873274087905884\n",
      "----------------------------------\n",
      "Epoch: 1010\n",
      "Total Loss: 0.18270368874073029\n",
      "----------------------------------\n",
      "Epoch: 1020\n",
      "Total Loss: 0.2217196673154831\n",
      "----------------------------------\n",
      "Epoch: 1030\n",
      "Total Loss: 0.19277504086494446\n",
      "----------------------------------\n",
      "Epoch: 1040\n",
      "Total Loss: 0.21201132237911224\n",
      "----------------------------------\n",
      "Epoch: 1050\n",
      "Total Loss: 0.2920099198818207\n",
      "----------------------------------\n",
      "Epoch: 1060\n",
      "Total Loss: 0.2221508026123047\n",
      "----------------------------------\n",
      "Epoch: 1070\n",
      "Total Loss: 0.1788358837366104\n",
      "----------------------------------\n",
      "Epoch: 1080\n",
      "Total Loss: 0.2035062611103058\n",
      "----------------------------------\n",
      "Epoch: 1090\n",
      "Total Loss: 0.20565584301948547\n",
      "----------------------------------\n",
      "Epoch: 1100\n",
      "Total Loss: 0.17259174585342407\n",
      "----------------------------------\n",
      "Epoch: 1110\n",
      "Total Loss: 0.33788245916366577\n",
      "----------------------------------\n",
      "Epoch: 1120\n",
      "Total Loss: 0.26257094740867615\n",
      "----------------------------------\n",
      "Epoch: 1130\n",
      "Total Loss: 0.290901780128479\n",
      "----------------------------------\n",
      "Epoch: 1140\n",
      "Total Loss: 0.16649435460567474\n",
      "----------------------------------\n",
      "Epoch: 1150\n",
      "Total Loss: 0.20373046398162842\n",
      "----------------------------------\n",
      "Epoch: 1160\n",
      "Total Loss: 0.23943091928958893\n",
      "----------------------------------\n",
      "Epoch: 1170\n",
      "Total Loss: 0.2405131757259369\n",
      "----------------------------------\n",
      "Epoch: 1180\n",
      "Total Loss: 0.1730894297361374\n",
      "----------------------------------\n",
      "Epoch: 1190\n",
      "Total Loss: 0.19677788019180298\n",
      "----------------------------------\n",
      "Epoch: 1200\n",
      "Total Loss: 0.1834341287612915\n",
      "----------------------------------\n",
      "Epoch: 1210\n",
      "Total Loss: 0.17822790145874023\n",
      "----------------------------------\n",
      "Epoch: 1220\n",
      "Total Loss: 0.1762746274471283\n",
      "----------------------------------\n",
      "Epoch: 1230\n",
      "Total Loss: 0.17237423360347748\n",
      "----------------------------------\n",
      "Epoch: 1240\n",
      "Total Loss: 0.16151940822601318\n",
      "----------------------------------\n",
      "Epoch: 1250\n",
      "Total Loss: 0.20070630311965942\n",
      "----------------------------------\n",
      "Epoch: 1260\n",
      "Total Loss: 0.19578245282173157\n",
      "----------------------------------\n",
      "Epoch: 1270\n",
      "Total Loss: 0.16440334916114807\n",
      "----------------------------------\n",
      "Epoch: 1280\n",
      "Total Loss: 0.1390380710363388\n",
      "----------------------------------\n",
      "Epoch: 1290\n",
      "Total Loss: 0.1661367416381836\n",
      "----------------------------------\n",
      "Epoch: 1300\n",
      "Total Loss: 0.13464976847171783\n",
      "----------------------------------\n",
      "Epoch: 1310\n",
      "Total Loss: 0.17282800376415253\n",
      "----------------------------------\n",
      "Epoch: 1320\n",
      "Total Loss: 0.12301310896873474\n",
      "----------------------------------\n",
      "Epoch: 1330\n",
      "Total Loss: 0.19901233911514282\n",
      "----------------------------------\n",
      "Epoch: 1340\n",
      "Total Loss: 0.15237730741500854\n",
      "----------------------------------\n",
      "Epoch: 1350\n",
      "Total Loss: 0.1385263055562973\n",
      "----------------------------------\n",
      "Epoch: 1360\n",
      "Total Loss: 0.14613690972328186\n",
      "----------------------------------\n",
      "Epoch: 1370\n",
      "Total Loss: 0.14721408486366272\n",
      "----------------------------------\n",
      "Epoch: 1380\n",
      "Total Loss: 0.1958373785018921\n",
      "----------------------------------\n",
      "Epoch: 1390\n",
      "Total Loss: 0.1105789765715599\n",
      "----------------------------------\n",
      "Epoch: 1400\n",
      "Total Loss: 0.12694601714611053\n",
      "----------------------------------\n",
      "Epoch: 1410\n",
      "Total Loss: 0.12666656076908112\n",
      "----------------------------------\n",
      "Epoch: 1420\n",
      "Total Loss: 0.1340760737657547\n",
      "----------------------------------\n",
      "Epoch: 1430\n",
      "Total Loss: 0.1884230375289917\n",
      "----------------------------------\n",
      "Epoch: 1440\n",
      "Total Loss: 0.17051443457603455\n",
      "----------------------------------\n",
      "Epoch: 1450\n",
      "Total Loss: 0.1371079385280609\n",
      "----------------------------------\n",
      "Epoch: 1460\n",
      "Total Loss: 0.22112524509429932\n",
      "----------------------------------\n",
      "Epoch: 1470\n",
      "Total Loss: 0.19797104597091675\n",
      "----------------------------------\n",
      "Epoch: 1480\n",
      "Total Loss: 0.12897907197475433\n",
      "----------------------------------\n",
      "Epoch: 1490\n",
      "Total Loss: 0.17016024887561798\n",
      "----------------------------------\n",
      "Epoch: 1500\n",
      "Total Loss: 0.13082273304462433\n",
      "----------------------------------\n",
      "Epoch: 1510\n",
      "Total Loss: 0.12866927683353424\n",
      "----------------------------------\n",
      "Epoch: 1520\n",
      "Total Loss: 0.130298912525177\n",
      "----------------------------------\n",
      "Epoch: 1530\n",
      "Total Loss: 0.11583829671144485\n",
      "----------------------------------\n",
      "Epoch: 1540\n",
      "Total Loss: 0.18245486915111542\n",
      "----------------------------------\n",
      "Epoch: 1550\n",
      "Total Loss: 0.08513358235359192\n",
      "----------------------------------\n",
      "Epoch: 1560\n",
      "Total Loss: 0.1504770964384079\n",
      "----------------------------------\n",
      "Epoch: 1570\n",
      "Total Loss: 0.09652545303106308\n",
      "----------------------------------\n",
      "Epoch: 1580\n",
      "Total Loss: 0.17023450136184692\n",
      "----------------------------------\n",
      "Epoch: 1590\n",
      "Total Loss: 0.18877393007278442\n",
      "----------------------------------\n",
      "Epoch: 1600\n",
      "Total Loss: 0.10259012132883072\n",
      "----------------------------------\n",
      "Epoch: 1610\n",
      "Total Loss: 0.15631712973117828\n",
      "----------------------------------\n",
      "Epoch: 1620\n",
      "Total Loss: 0.13115638494491577\n",
      "----------------------------------\n",
      "Epoch: 1630\n",
      "Total Loss: 0.14290758967399597\n",
      "----------------------------------\n",
      "Epoch: 1640\n",
      "Total Loss: 0.17589370906352997\n",
      "----------------------------------\n",
      "Epoch: 1650\n",
      "Total Loss: 0.10308612883090973\n",
      "----------------------------------\n",
      "Epoch: 1660\n",
      "Total Loss: 0.14579039812088013\n",
      "----------------------------------\n",
      "Epoch: 1670\n",
      "Total Loss: 0.14494624733924866\n",
      "----------------------------------\n",
      "Epoch: 1680\n",
      "Total Loss: 0.11727578192949295\n",
      "----------------------------------\n",
      "Epoch: 1690\n",
      "Total Loss: 0.18225038051605225\n",
      "----------------------------------\n",
      "Epoch: 1700\n",
      "Total Loss: 0.08966371417045593\n",
      "----------------------------------\n",
      "Epoch: 1710\n",
      "Total Loss: 0.1047217920422554\n",
      "----------------------------------\n",
      "Epoch: 1720\n",
      "Total Loss: 0.1306854784488678\n",
      "----------------------------------\n",
      "Epoch: 1730\n",
      "Total Loss: 0.13528092205524445\n",
      "----------------------------------\n",
      "Epoch: 1740\n",
      "Total Loss: 0.09588368237018585\n",
      "----------------------------------\n",
      "Epoch: 1750\n",
      "Total Loss: 0.1519685536623001\n",
      "----------------------------------\n",
      "Epoch: 1760\n",
      "Total Loss: 0.09273011237382889\n",
      "----------------------------------\n",
      "Epoch: 1770\n",
      "Total Loss: 0.09778118878602982\n",
      "----------------------------------\n",
      "Epoch: 1780\n",
      "Total Loss: 0.12834040820598602\n",
      "----------------------------------\n",
      "Epoch: 1790\n",
      "Total Loss: 0.09322147071361542\n",
      "----------------------------------\n",
      "Epoch: 1800\n",
      "Total Loss: 0.18857912719249725\n",
      "----------------------------------\n",
      "Epoch: 1810\n",
      "Total Loss: 0.06938865035772324\n",
      "----------------------------------\n",
      "Epoch: 1820\n",
      "Total Loss: 0.11847829073667526\n",
      "----------------------------------\n",
      "Epoch: 1830\n",
      "Total Loss: 0.1704152226448059\n",
      "----------------------------------\n",
      "Epoch: 1840\n",
      "Total Loss: 0.1323907971382141\n",
      "----------------------------------\n",
      "Epoch: 1850\n",
      "Total Loss: 0.14027832448482513\n",
      "----------------------------------\n",
      "Epoch: 1860\n",
      "Total Loss: 0.1257384717464447\n",
      "----------------------------------\n",
      "Epoch: 1870\n",
      "Total Loss: 0.24592775106430054\n",
      "----------------------------------\n",
      "Epoch: 1880\n",
      "Total Loss: 0.15222664177417755\n",
      "----------------------------------\n",
      "Epoch: 1890\n",
      "Total Loss: 0.11771102249622345\n",
      "----------------------------------\n",
      "Epoch: 1900\n",
      "Total Loss: 0.2016134411096573\n",
      "----------------------------------\n",
      "Epoch: 1910\n",
      "Total Loss: 0.08491697162389755\n",
      "----------------------------------\n",
      "Epoch: 1920\n",
      "Total Loss: 0.06825560331344604\n",
      "----------------------------------\n",
      "Epoch: 1930\n",
      "Total Loss: 0.11012662202119827\n",
      "----------------------------------\n",
      "Epoch: 1940\n",
      "Total Loss: 0.16454219818115234\n",
      "----------------------------------\n",
      "Epoch: 1950\n",
      "Total Loss: 0.18244537711143494\n",
      "----------------------------------\n",
      "Epoch: 1960\n",
      "Total Loss: 0.10614574700593948\n",
      "----------------------------------\n",
      "Epoch: 1970\n",
      "Total Loss: 0.10386686027050018\n",
      "----------------------------------\n",
      "Epoch: 1980\n",
      "Total Loss: 0.13322333991527557\n",
      "----------------------------------\n",
      "Epoch: 1990\n",
      "Total Loss: 0.1635035127401352\n",
      "----------------------------------\n",
      "Epoch: 2000\n",
      "Total Loss: 0.06985335797071457\n",
      "----------------------------------\n",
      "Epoch: 2010\n",
      "Total Loss: 0.1509043276309967\n",
      "----------------------------------\n",
      "Epoch: 2020\n",
      "Total Loss: 0.10349158197641373\n",
      "----------------------------------\n",
      "Epoch: 2030\n",
      "Total Loss: 0.08296272158622742\n",
      "----------------------------------\n",
      "Epoch: 2040\n",
      "Total Loss: 0.11451660096645355\n",
      "----------------------------------\n",
      "Epoch: 2050\n",
      "Total Loss: 0.12182392179965973\n",
      "----------------------------------\n",
      "Epoch: 2060\n",
      "Total Loss: 0.08780404180288315\n",
      "----------------------------------\n",
      "Epoch: 2070\n",
      "Total Loss: 0.07356634736061096\n",
      "----------------------------------\n",
      "Epoch: 2080\n",
      "Total Loss: 0.10806931555271149\n",
      "----------------------------------\n",
      "Epoch: 2090\n",
      "Total Loss: 0.06803757697343826\n",
      "----------------------------------\n",
      "Epoch: 2100\n",
      "Total Loss: 0.07862608134746552\n",
      "----------------------------------\n",
      "Epoch: 2110\n",
      "Total Loss: 0.11827044188976288\n",
      "----------------------------------\n",
      "Epoch: 2120\n",
      "Total Loss: 0.07894393056631088\n",
      "----------------------------------\n",
      "Epoch: 2130\n",
      "Total Loss: 0.07093360275030136\n",
      "----------------------------------\n",
      "Epoch: 2140\n",
      "Total Loss: 0.10922231525182724\n",
      "----------------------------------\n",
      "Epoch: 2150\n",
      "Total Loss: 0.15487433969974518\n",
      "----------------------------------\n",
      "Epoch: 2160\n",
      "Total Loss: 0.08020101487636566\n",
      "----------------------------------\n",
      "Epoch: 2170\n",
      "Total Loss: 0.11391381174325943\n",
      "----------------------------------\n",
      "Epoch: 2180\n",
      "Total Loss: 0.1148194894194603\n",
      "----------------------------------\n",
      "Epoch: 2190\n",
      "Total Loss: 0.13977377116680145\n",
      "----------------------------------\n",
      "Epoch: 2200\n",
      "Total Loss: 0.07278408855199814\n",
      "----------------------------------\n",
      "Epoch: 2210\n",
      "Total Loss: 0.07990960031747818\n",
      "----------------------------------\n",
      "Epoch: 2220\n",
      "Total Loss: 0.15500523149967194\n",
      "----------------------------------\n",
      "Epoch: 2230\n",
      "Total Loss: 0.1362258493900299\n",
      "----------------------------------\n",
      "Epoch: 2240\n",
      "Total Loss: 0.03554857522249222\n",
      "----------------------------------\n",
      "Epoch: 2250\n",
      "Total Loss: 0.05456971749663353\n",
      "----------------------------------\n",
      "Epoch: 2260\n",
      "Total Loss: 0.10202871263027191\n",
      "----------------------------------\n",
      "Epoch: 2270\n",
      "Total Loss: 0.07069142162799835\n",
      "----------------------------------\n",
      "Epoch: 2280\n",
      "Total Loss: 0.04453936591744423\n",
      "----------------------------------\n",
      "Epoch: 2290\n",
      "Total Loss: 0.0737546905875206\n",
      "----------------------------------\n",
      "Epoch: 2300\n",
      "Total Loss: 0.09095220267772675\n",
      "----------------------------------\n",
      "Epoch: 2310\n",
      "Total Loss: 0.08006460964679718\n",
      "----------------------------------\n",
      "Epoch: 2320\n",
      "Total Loss: 0.0649028941988945\n",
      "----------------------------------\n",
      "Epoch: 2330\n",
      "Total Loss: 0.1130327582359314\n",
      "----------------------------------\n",
      "Epoch: 2340\n",
      "Total Loss: 0.1797681599855423\n",
      "----------------------------------\n",
      "Epoch: 2350\n",
      "Total Loss: 0.054700013250112534\n",
      "----------------------------------\n",
      "Epoch: 2360\n",
      "Total Loss: 0.12606129050254822\n",
      "----------------------------------\n",
      "Epoch: 2370\n",
      "Total Loss: 0.09789780527353287\n",
      "----------------------------------\n",
      "Epoch: 2380\n",
      "Total Loss: 0.04665743187069893\n",
      "----------------------------------\n",
      "Epoch: 2390\n",
      "Total Loss: 0.06385598331689835\n",
      "----------------------------------\n",
      "Epoch: 2400\n",
      "Total Loss: 0.1314353346824646\n",
      "----------------------------------\n",
      "Epoch: 2410\n",
      "Total Loss: 0.10526782274246216\n",
      "----------------------------------\n",
      "Epoch: 2420\n",
      "Total Loss: 0.09165668487548828\n",
      "----------------------------------\n",
      "Epoch: 2430\n",
      "Total Loss: 0.10789694637060165\n",
      "----------------------------------\n",
      "Epoch: 2440\n",
      "Total Loss: 0.10886295139789581\n",
      "----------------------------------\n",
      "Epoch: 2450\n",
      "Total Loss: 0.17095829546451569\n",
      "----------------------------------\n",
      "Epoch: 2460\n",
      "Total Loss: 0.14996811747550964\n",
      "----------------------------------\n",
      "Epoch: 2470\n",
      "Total Loss: 0.08984968811273575\n",
      "----------------------------------\n",
      "Epoch: 2480\n",
      "Total Loss: 0.12570172548294067\n",
      "----------------------------------\n",
      "Epoch: 2490\n",
      "Total Loss: 0.1300043761730194\n",
      "----------------------------------\n",
      "Epoch: 2500\n",
      "Total Loss: 0.09415466338396072\n",
      "----------------------------------\n",
      "Epoch: 2510\n",
      "Total Loss: 0.12457793951034546\n",
      "----------------------------------\n",
      "Epoch: 2520\n",
      "Total Loss: 0.09211302548646927\n",
      "----------------------------------\n",
      "Epoch: 2530\n",
      "Total Loss: 0.05829924717545509\n",
      "----------------------------------\n",
      "Epoch: 2540\n",
      "Total Loss: 0.0961604192852974\n",
      "----------------------------------\n",
      "Epoch: 2550\n",
      "Total Loss: 0.064938023686409\n",
      "----------------------------------\n",
      "Epoch: 2560\n",
      "Total Loss: 0.08768212050199509\n",
      "----------------------------------\n",
      "Epoch: 2570\n",
      "Total Loss: 0.0709167867898941\n",
      "----------------------------------\n",
      "Epoch: 2580\n",
      "Total Loss: 0.086094930768013\n",
      "----------------------------------\n",
      "Epoch: 2590\n",
      "Total Loss: 0.049169186502695084\n",
      "----------------------------------\n",
      "Epoch: 2600\n",
      "Total Loss: 0.04916779696941376\n",
      "----------------------------------\n",
      "Epoch: 2610\n",
      "Total Loss: 0.09459526091814041\n",
      "----------------------------------\n",
      "Epoch: 2620\n",
      "Total Loss: 0.05244750156998634\n",
      "----------------------------------\n",
      "Epoch: 2630\n",
      "Total Loss: 0.08715758472681046\n",
      "----------------------------------\n",
      "Epoch: 2640\n",
      "Total Loss: 0.11075874418020248\n",
      "----------------------------------\n",
      "Epoch: 2650\n",
      "Total Loss: 0.11410743743181229\n",
      "----------------------------------\n",
      "Epoch: 2660\n",
      "Total Loss: 0.051625169813632965\n",
      "----------------------------------\n",
      "Epoch: 2670\n",
      "Total Loss: 0.19162455201148987\n",
      "----------------------------------\n",
      "Epoch: 2680\n",
      "Total Loss: 0.06123794987797737\n",
      "----------------------------------\n",
      "Epoch: 2690\n",
      "Total Loss: 0.06043132394552231\n",
      "----------------------------------\n",
      "Epoch: 2700\n",
      "Total Loss: 0.10513808578252792\n",
      "----------------------------------\n",
      "Epoch: 2710\n",
      "Total Loss: 0.10772470384836197\n",
      "----------------------------------\n",
      "Epoch: 2720\n",
      "Total Loss: 0.07905898988246918\n",
      "----------------------------------\n",
      "Epoch: 2730\n",
      "Total Loss: 0.052032727748155594\n",
      "----------------------------------\n",
      "Epoch: 2740\n",
      "Total Loss: 0.03528987988829613\n",
      "----------------------------------\n",
      "Epoch: 2750\n",
      "Total Loss: 0.09262222051620483\n",
      "----------------------------------\n",
      "Epoch: 2760\n",
      "Total Loss: 0.11100537329912186\n",
      "----------------------------------\n",
      "Epoch: 2770\n",
      "Total Loss: 0.06978851556777954\n",
      "----------------------------------\n",
      "Epoch: 2780\n",
      "Total Loss: 0.11143682152032852\n",
      "----------------------------------\n",
      "Epoch: 2790\n",
      "Total Loss: 0.05019228532910347\n",
      "----------------------------------\n",
      "Epoch: 2800\n",
      "Total Loss: 0.13045859336853027\n",
      "----------------------------------\n",
      "Epoch: 2810\n",
      "Total Loss: 0.05666826292872429\n",
      "----------------------------------\n",
      "Epoch: 2820\n",
      "Total Loss: 0.08970029652118683\n",
      "----------------------------------\n",
      "Epoch: 2830\n",
      "Total Loss: 0.06759215146303177\n",
      "----------------------------------\n",
      "Epoch: 2840\n",
      "Total Loss: 0.06987231224775314\n",
      "----------------------------------\n",
      "Epoch: 2850\n",
      "Total Loss: 0.06466348469257355\n",
      "----------------------------------\n",
      "Epoch: 2860\n",
      "Total Loss: 0.11172357201576233\n",
      "----------------------------------\n",
      "Epoch: 2870\n",
      "Total Loss: 0.08691181242465973\n",
      "----------------------------------\n",
      "Epoch: 2880\n",
      "Total Loss: 0.05001400411128998\n",
      "----------------------------------\n",
      "Epoch: 2890\n",
      "Total Loss: 0.0652586966753006\n",
      "----------------------------------\n",
      "Epoch: 2900\n",
      "Total Loss: 0.07726559042930603\n",
      "----------------------------------\n",
      "Epoch: 2910\n",
      "Total Loss: 0.06549801677465439\n",
      "----------------------------------\n",
      "Epoch: 2920\n",
      "Total Loss: 0.048437345772981644\n",
      "----------------------------------\n",
      "Epoch: 2930\n",
      "Total Loss: 0.1532323807477951\n",
      "----------------------------------\n",
      "Epoch: 2940\n",
      "Total Loss: 0.04062338173389435\n",
      "----------------------------------\n",
      "Epoch: 2950\n",
      "Total Loss: 0.06092115864157677\n",
      "----------------------------------\n",
      "Epoch: 2960\n",
      "Total Loss: 0.12061576545238495\n",
      "----------------------------------\n",
      "Epoch: 2970\n",
      "Total Loss: 0.07836759090423584\n",
      "----------------------------------\n",
      "Epoch: 2980\n",
      "Total Loss: 0.04866607487201691\n",
      "----------------------------------\n",
      "Epoch: 2990\n",
      "Total Loss: 0.05048404633998871\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3000\n",
    "best_model, loss_, epochs = train_loop(model, n_epochs, data_loader.data, data_loader.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, loss_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_seq, max_length=5, SOS_token=5, EOS_token=6, verbose=False):\n",
    "    model.eval()\n",
    "\n",
    "    target_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "    # For graphing attention\n",
    "    #encoder_outputs = model.positional_encoder(input_seq)\n",
    "    #attentions = torch.zeros(30, 1, len(input_seq)).to(device)\n",
    "\n",
    "    # Asks model to give only one item, the next thing it thinks is most probable \n",
    "    # to continue the sentence\n",
    "    for i in range(max_length):\n",
    "        sequence_length = target_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "        src_mask = model.get_tgt_mask(input_seq.size(1)).to(device)\n",
    "        \n",
    "        #attentions[i] = model.decoder.one_step_decoder(encoder_outputs)\n",
    "        \n",
    "        #print(input_seq)\n",
    "        pred = model(input_seq, target_input, src_mask, tgt_mask)\n",
    "        \n",
    "        pred = pred.permute(2, 0, 1)\n",
    "        \n",
    "        if verbose:\n",
    "            print(pred.cpu().squeeze().detach().numpy())\n",
    "            print(pred.topk(1))\n",
    "            print(pred.topk(1)[1].view(-1)[-1].item())\n",
    "        \n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability, twas view(-1)[-1] before\n",
    "    \n",
    "        next_item = torch.tensor([[next_item]], device=device)\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        target_input = torch.cat((target_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "        elif (len(target_input.view(-1)) >= len(input_seq[0])):\n",
    "            break\n",
    "\n",
    "    return target_input.view(-1).tolist()\n",
    "\n",
    "def test_example(model, example, data_loader, i):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]} --> Prediction: {result[1:-1]}  vs Actual: {data_loader.target[i].view(-1).tolist()[1:-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moment of Truth; Has it at least memorized the data?\n",
      "------------------------------------\n",
      "Input: [0, 0, 0] --> Prediction: [0, 0, 0]  vs Actual: [0, 0, 0]\n",
      "\n",
      "Input: [0, 0, 1] --> Prediction: [1, 1, 0]  vs Actual: [1, 1, 0]\n",
      "\n",
      "Input: [0, 0, 2] --> Prediction: [2, 2, 0]  vs Actual: [2, 2, 0]\n",
      "\n",
      "Input: [0, 1, 1] --> Prediction: [2, 1, 1]  vs Actual: [2, 1, 1]\n",
      "\n",
      "Input: [0, 1, 2] --> Prediction: [3, 2, 1]  vs Actual: [3, 2, 1]\n",
      "\n",
      "Input: [0, 2, 0] --> Prediction: [2, 0, 2]  vs Actual: [2, 0, 2]\n",
      "\n",
      "Input: [0, 2, 1] --> Prediction: [3, 1, 2]  vs Actual: [3, 1, 2]\n",
      "\n",
      "Input: [0, 2, 2] --> Prediction: [4, 2, 2]  vs Actual: [4, 2, 2]\n",
      "\n",
      "Input: [1, 0, 0] --> Prediction: [0, 1, 1]  vs Actual: [0, 1, 1]\n",
      "\n",
      "Input: [1, 0, 1] --> Prediction: [1, 2, 1]  vs Actual: [1, 2, 1]\n",
      "\n",
      "Input: [1, 0, 2] --> Prediction: [2, 3, 1]  vs Actual: [2, 3, 1]\n",
      "\n",
      "Input: [1, 1, 0] --> Prediction: [1, 1, 2]  vs Actual: [1, 1, 2]\n",
      "\n",
      "Input: [1, 1, 1] --> Prediction: [2, 2, 2]  vs Actual: [2, 2, 2]\n",
      "\n",
      "Input: [1, 1, 2] --> Prediction: [3, 3, 2]  vs Actual: [3, 3, 2]\n",
      "\n",
      "Input: [1, 2, 1] --> Prediction: [3, 2, 3]  vs Actual: [3, 2, 3]\n",
      "\n",
      "Input: [1, 2, 2] --> Prediction: [4, 3, 3]  vs Actual: [4, 3, 3]\n",
      "\n",
      "Input: [2, 0, 0] --> Prediction: [0, 2, 2]  vs Actual: [0, 2, 2]\n",
      "\n",
      "Input: [2, 0, 1] --> Prediction: [1, 3, 2]  vs Actual: [1, 3, 2]\n",
      "\n",
      "Input: [2, 0, 2] --> Prediction: [2, 4, 2]  vs Actual: [2, 4, 2]\n",
      "\n",
      "Input: [2, 1, 0] --> Prediction: [1, 2, 3]  vs Actual: [1, 2, 3]\n",
      "\n",
      "Input: [2, 1, 1] --> Prediction: [2, 3, 3]  vs Actual: [2, 3, 3]\n",
      "\n",
      "Input: [2, 1, 2] --> Prediction: [3, 4, 3]  vs Actual: [3, 4, 3]\n",
      "\n",
      "Input: [2, 2, 0] --> Prediction: [2, 2, 4]  vs Actual: [2, 2, 4]\n",
      "\n",
      "Input: [2, 2, 1] --> Prediction: [3, 3, 4]  vs Actual: [3, 3, 4]\n",
      "\n",
      "Input: [2, 2, 2] --> Prediction: [4, 4, 4]  vs Actual: [4, 4, 4]\n",
      "\n",
      "Did it maybe possible work? Ich würde viele Aufregung haben, als das passieren würde!\n",
      "Let uns check the Wahrscheinlichkeiten, yo: \n",
      "[-1.8554677  -7.4746804   0.43262005  1.6631914   9.675396    0.3949233\n",
      " -1.9553292 ]\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[9.6754]]], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[[4]]]))\n",
      "4\n",
      "[[-1.8554677  -7.4746814   0.43261957  1.6631916   9.675396    0.3949234\n",
      "  -1.955329  ]\n",
      " [-2.406394   -7.71811     0.68734026  1.6530408   9.286783   -0.11899293\n",
      "  -0.49892935]]\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[9.6754]],\n",
      "\n",
      "        [[9.2868]]], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[[4]],\n",
      "\n",
      "        [[4]]]))\n",
      "4\n",
      "[[-1.8554677  -7.4746804   0.4326194   1.6631923   9.675396    0.39492336\n",
      "  -1.9553294 ]\n",
      " [-2.406394   -7.718111    0.68733954  1.65304     9.286783   -0.11899293\n",
      "  -0.4989287 ]\n",
      " [-2.5982852  -8.187229   -0.2053145   0.15879236  8.11203    -0.38219655\n",
      "   4.017741  ]]\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[9.6754]],\n",
      "\n",
      "        [[9.2868]],\n",
      "\n",
      "        [[8.1120]]], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[[4]],\n",
      "\n",
      "        [[4]],\n",
      "\n",
      "        [[4]]]))\n",
      "4\n",
      "[[-1.8554679  -7.474681    0.4326195   1.6631917   9.675396    0.39492348\n",
      "  -1.9553286 ]\n",
      " [-2.4063938  -7.718111    0.6873391   1.6530403   9.286783   -0.1189929\n",
      "  -0.4989285 ]\n",
      " [-2.598285   -8.187228   -0.20531453  0.15879105  8.11203    -0.3821964\n",
      "   4.0177426 ]\n",
      " [-2.738061   -7.530668   -0.7828561  -0.8090718   5.580314   -0.63906956\n",
      "   7.6840634 ]]\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[9.6754]],\n",
      "\n",
      "        [[9.2868]],\n",
      "\n",
      "        [[8.1120]],\n",
      "\n",
      "        [[7.6841]]], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[[4]],\n",
      "\n",
      "        [[4]],\n",
      "\n",
      "        [[4]],\n",
      "\n",
      "        [[6]]]))\n",
      "6\n",
      "Example\n",
      "Input: [2, 2, 2] --> Continutation: [4, 4, 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The reckoning\n",
    "print('Moment of Truth; Has it at least memorized the data?')\n",
    "print('------------------------------------')\n",
    "\n",
    "for i, sequence in enumerate(data_loader.data):\n",
    "    sequence = torch.unsqueeze(sequence, dim=0)\n",
    "    test_example(best_model, sequence, data_loader, i)\n",
    "print('Did it maybe possible work? Ich würde viele Aufregung haben, als das passieren würde!')\n",
    "\n",
    "# This time, I removed 0, 1, 0 from the dataset and added 2, 2, 2 to the set\n",
    "print('Let uns check the Wahrscheinlichkeiten, yo: ')\n",
    "example = torch.tensor([[5, 2, 2, 2, 6]]).type(torch.long).to(device)\n",
    "result = predict(model, example, verbose=True)\n",
    "print(f\"Example\")\n",
    "print(f\"Input: {example.view(-1).tolist()[1:-1]} --> Continutation: {result[1:-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 60])\n",
      "tensor([ 0.3698,  1.1046, -0.9318, -0.9205, -2.2954,  0.9863,  0.6419,  0.4080,\n",
      "         0.8408,  1.2128, -0.1428, -0.3307, -0.5541, -0.2497, -0.0855, -0.8298,\n",
      "        -0.0980,  0.6834, -0.6994,  0.5046,  0.1249,  0.2343, -0.2182,  0.1868,\n",
      "        -0.3454, -0.3038, -0.2618, -0.2291,  0.2313,  1.4082, -0.1149,  0.4035,\n",
      "         0.8224, -1.2948, -0.0192,  1.1119, -0.1439,  0.5857, -0.3925, -0.4322,\n",
      "         2.6767,  1.2234, -0.5463,  0.2799, -0.4363, -0.9570,  0.7650,  0.7426,\n",
      "         0.1974, -0.0349,  1.1887,  0.5518, -0.2866, -1.0540, -0.3265,  0.9813,\n",
      "         0.7314, -2.5228,  0.4350,  0.5059], grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.3698,  1.1046, -0.9318, -0.9205, -2.2954,  0.9863,  0.6419,  0.4080,\n",
      "          0.8408,  1.2128, -0.1428, -0.3307, -0.5541, -0.2497, -0.0855, -0.8298,\n",
      "         -0.0980,  0.6834, -0.6994,  0.5046,  0.1249,  0.2343, -0.2182,  0.1868,\n",
      "         -0.3454, -0.3038, -0.2618, -0.2291,  0.2313,  1.4082, -0.1149,  0.4035,\n",
      "          0.8224, -1.2948, -0.0192,  1.1119, -0.1439,  0.5857, -0.3925, -0.4322,\n",
      "          2.6767,  1.2234, -0.5463,  0.2799, -0.4363, -0.9570,  0.7650,  0.7426,\n",
      "          0.1974, -0.0349,  1.1887,  0.5518, -0.2866, -1.0540, -0.3265,  0.9813,\n",
      "          0.7314, -2.5228,  0.4350,  0.5059]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(best_model.embedding.weight.size())\n",
    "print(best_model.embedding.weight[0])\n",
    "print(best_model.embedding(torch.tensor([0]).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5288/1644857235.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdisplay_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5288/1644857235.py\u001b[0m in \u001b[0;36mdisplay_attention\u001b[0;34m(sentence, translation, attention)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     ax.set_xticklabels([''] + [t.lower() for t in sentence] + [''],\n\u001b[0m\u001b[1;32m     13\u001b[0m                        rotation=45)\n\u001b[1;32m     14\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_yticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtranslation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5288/1644857235.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     ax.set_xticklabels([''] + [t.lower() for t in sentence] + [''],\n\u001b[0m\u001b[1;32m     13\u001b[0m                        rotation=45)\n\u001b[1;32m     14\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_yticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtranslation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'lower'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAABECAYAAACcagG5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAK6klEQVR4nO3de4xcZRnH8e9vt91utze2CwbkYhcBayHGhIakIBDbeGkDSBoQBVRCpBISQKuCyEVESbgECAGlYiSoiAhIgiK1kasiGmwlQUsvpHQLCrG0bIWy0NL28Y9zFqbDds/peObM7Ozvk5zsznvefc8zz5zdfWfOO88oIjAzMzOz4bU1OgAzMzOzkcCTJjMzM7McPGkyMzMzy8GTJjMzM7McPGkyMzMzy8GTJjMzM7McSps0SZoh6WFJA5JeknSFpPayjt/qJB0k6UeSnpG0XdJjQ/SRpG9LelHSm5L+KOmj5Uc78kk6WdJvJP1b0mZJyyR9foh+Z0l6TtJbaZ85jYi3FUg6SdKTkjam+Vwl6RJJHRV9fI7XkaR90/M9JE2saHfeCyLpjDS/1dvZFX2c7wYpZdIkqRt4CAjgM8AVwNeB75Zx/FHiUGAesApYvYs+3wIuBa4Gjgc2Aw9J2ruUCFvLQpL8fQ04AXgUuFPSuYMd0knUIuBnwFxgOfCApMPKD7cl9ACPAF8myedtwMXA9RV9fI7X17UkOa3mvBdvNjCrYruvYp/z3SgRUfcNuAjoByZXtF0ADFS2efu/ctxW8f29wGNV+zuB/wKXVbRNAF4Bvt/o+EfaBuw5RNudwNqK26uA2yofI+AfwB2Njr9VNuBKYBMgn+N1z/UxwKvAN0ieAE9M2533YvN8RmV+h9jvfDdwK+vy3FxgSUS8VtF2FzAeOLakGFpaROzI6HIkMBm4u+Jn3gB+S/L42G6IiA1DND8NvB9A0oHAIeyc7x3APTjfRdoIDF6e8zleJ+lSiptIrhJUn/vOe7mc7wYqa9I0HVhZ2RARL5C80jS9pBhGu+nAduC5qvYV+DEoyizevTQ6mNOVVX1WAFMl7VVaVC1GUrukLkkfA84Dbonk6bbP8fo5GxgH/GCIfc57fayRtC1du/eVinbnu4HGlHScbpKX0Kv1p/us/rqBzRGxvaq9H+iS1BERWxsQV0tIF3ifCJyZNg2e15uquvZX7H+l7oG1pjdI/oFDsl7sm+n3PsfrQFIP8D3g9Ih4W1J1F+e9WC+TrFd6CmgHPgcsktQVETfgfDdUWZMms5YlaRrJeqb7I+L2xkYzKhwJdAFHAJcBNwPnNDSi1nYl8NeIeLDRgYwGEbEEWFLRtFhSJ3CJpBsbFJalypo09QNThmjv5t1n3lZf/cBESe1Vz1C6gQE/M6mNpKnAYmAdcFrFrsHzego7v9rUXbXfdlNE/D399glJG4CfSroOn+OFk3Qoyaunx0jaI23uSr9OkbQd570M9wKfBabhfDdUWWuaVlJ1rVXS/iS/fNVrPqw+VpK81HtQVft71ptZPpK6gAdIFiIfFxEDFbsHc1q9xmA68GpE+NJcMQYnUL34HK+Hg4GxwF9I/ln38+66pn+RLA533usvKr463w1U1qRpMfApSZMq2k4B3gQeLymG0e5J4DXg5MGG9J/+8SSPj+0GSWNI3gl3MPDpiFhfuT8inidZFF6Z77b0tvNdnKPSr2vxOV4PTwAfr9quTvfNI6nb5LzX30kk71pch/PdUGVdnltE8i6X+yRdDRwIXA5cX1WGwGqU/tLMS2/uC0yWdFJ6+8GIGJB0FXCppH6SZyQLSSbON5Ue8Mj3Q5J8nw/0pItlBz0dEVtIzvE7JPUBfwa+RDLJOrXcUFuDpN+TFMldTvLuoaNIiuT+KiLWpH18jhcoLa3xWGVbuoYP4E8RsTltc94LIunXJIvAnyF5RemUdDsvLVvylvPdOKVMmiKiP3130c0ktSQ2ATeQ/FOxYryP5JWPSoO3e4E+4CqSX6yLSKorLwU+ERH/KSnGVvLJ9OtQCzN7gb6I+GX6URMXkrwbZjnJZbx/lhRjq/kbSeG/acA24HmSc3lRRR+f443hvBdnFck6sv1JirY+C3wxIn5e0cf5bhAl5U3MzMzMbDilfWCvmZmZ2UjmSZOZmZlZDp40mZmZmeXgSZOZmZlZDp40mZmZmeXgSZOZmZlZDqVPmiQtKPuYo51zXj7nvHzOefmc8/I5543ViFea/ICXzzkvn3NePue8fM55+ZzzBso1aZI0Q9LDkgYkvSTpCknt9Q7OzMzMrFlkVgSX1E3y8Q/PknxQ4weB64AbIuKSrAN0dHRGZ+eEd25v3foWHR2dO/U55JDeXMGuXr02s0+esYoap+yYah3LOa99nFrHcs5rH6fWsZzz2sepdSznvPZxah3LOa99nLxjLVu2bENE7DXUvjyfPXc2MB6Yn3647h8kTQYul3RN1gfudnZOYObMucMe4JFH7sgRBsyefXpmnzxjFTVO2TEVOVYz3r9mjKnIsZrx/jVjTEWO1Yz3rxljKnKsZrx/zRhTkWM14/1rxpjyjiVp3a725bk8NxdYUjU5uotkInVsjp83MzMzG/HyTJqmAysrGyLiBWAg3WdmZmbW8vJcnusGNg3R3p/ue4/0LZELAMaN66o1NjMzM7OmUZeSAxFxa0TMjIiZ1QvWzMzMzEaiPJOmfmDKEO3d6T4zMzOzlpdn0rSSqrVLkvYHuqha62RmZmbWqvKsaVoMXCjpNmAmcCjQB7wJPJ71wwf07sctv7h22D633L84Rxhw7jVfzezz+IoVmX3mn3NKZp8169fnCYmjT5iT2WfD669n9vnIrMNzHW9gy5bMPr3TP5TZZ+u2bbmOt89+B2T22b5jR2afnp69M/vsyKgZNmjSpKm5+mXp6ppUyDhQ7Nq9sWPHFTLOmDFjCxkHoL29mFq2bW3FrQgocixJTTWOmTWnPH91FgE7gFNJLsetB6YB12fVaDIzMzNrFZmTpojoB44GngSOAPYAXgS+U9fIzMzMzJpInstzRMRyYDaApHuBPSNiez0DMzMzM2smdSk5YGZmZtZq6jJpkrRA0lJJS/s3bqzHIczMzMxKVffilt09PfU4hJmZmVmpfHnOzMzMLAdPmszMzMxyyHz3nKSTgS8Ah5N8nMrbwMt1jsvMzMysqeQpObAQeAG4C3gdWAB8WNJPSKqFPxgRA7s8QFsb3ROGr5Z8zonzcgUbOSpGz5+/MLPPjT++OLPP3b97NFdMR82ZmdnnqTVrMvvMmDUj1/H6NmzI7NN7WG9mn00Db+Q63t4H7pPZZ2Dr1sw+3ftkV/Hetj1fFYvJU4f6KMSd5alSPn7CxFzHy1OpfNy48bnGyqOjo6iK4B2FjAPQ1parOkmm9oLGgWKrbzdjRfBWjslspMrzF+x4YCKwtqr9zHTrJflYFTMzM7OWlaci+IaI6IsIDW7ABcDW9HZf3aM0MzMza7BaF4LPAlYXGYiZmZlZM9vtBQaS5gAnklyaMzMzMxsVduuVJknTgDuB+yPi9mH6vVMRfKMrgpuZmVkLyD1pkjSV5N1y64DThutbWRG8xxXBzczMrAXkmjRJ6gIeADqA44YrMWBmZmbWivIUtxwD3AMcDBwZEevrHpWZmZlZk1FWwUhJtwJnAecDT1XtfjoitmT8/Cskl/QG7QlkV2i0Ijnn5XPOy+ecl885L59zXn8fiIi9htqRZ9LUB3xgF7t7d7dOk6SlEZFdRtsK45yXzzkvn3NePue8fM55Y2VenouIaSXEYWZmZtbUai1uaWZmZjaqNGLSdGsDjjnaOeflc87L55yXzzkvn3PeQJlrmszMzMzMl+fMzMzMcvGkyczMzCwHT5rMzMzMcvCkyczMzCwHT5rMzMzMcvCkyczMzCyH/wHROTGh6gJKAAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize Attention\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()[:-1]\n",
    "\n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "\n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.set_xticklabels([''] + [t.lower() for t in sentence] + [''],\n",
    "                       rotation=45)\n",
    "    ax.set_yticklabels([''] + translation + [''])\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "display_attention(example, result, pos_encoding[:len(result) - 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
